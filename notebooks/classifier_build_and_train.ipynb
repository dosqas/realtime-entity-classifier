{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d269f49f-edda-4d60-9bb3-800e6f7d737a",
   "metadata": {},
   "source": [
    "# ğŸ§  Real-time Entity Classifier - CNN Architecture & Training\n",
    "\n",
    "This notebook handles both definition and training of a MobileNetV2-based CNN for real-time webcam classification:\n",
    "\n",
    "**Detection Categories**:\n",
    "- ğŸ± **Pet** (Cat)\n",
    "- ğŸ‘¤ **Owner**\n",
    "- ğŸ§ **Other People**\n",
    "- ğŸš« **Background/Nobody**\n",
    "\n",
    "**Key Features**:\n",
    "- âš¡ Real-time inference: ~30ms/frame (640x480 resolution)\n",
    "- ğŸ”’ Privacy-focused: All processing on-device\n",
    "- ğŸŒŸ Robust to: Lighting changes, partial occlusions\n",
    "- ğŸ—ï¸ Transfer learning: Fine-tuned from ImageNet weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7efa1e-0ec7-4f5a-8669-b8f858f56391",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d4b3a2-c363-4c1e-947b-a83b70150fb7",
   "metadata": {},
   "source": [
    "## ğŸ“¦ PyTorch & Project Imports Overview\n",
    "\n",
    "### ğŸ§  Core PyTorch Components\n",
    "\n",
    "- **`torch`**  \n",
    "  Used for tensor operations, device management (`torch.cuda.is_available()`), and model saving (`torch.save()`).\n",
    "\n",
    "- **`torch.nn`**  \n",
    "  Used for building the classifier component of our model, including `nn.Linear`, `nn.ReLU`, `nn.Dropout`, and `nn.Sequential`. Also provides `CrossEntropyLoss` with label smoothing to handle class imbalance.\n",
    "\n",
    "- **`torch.optim`**  \n",
    "  Provides the `Adam` optimizer with learning rate and weight decay parameters for training our classifier layers.\n",
    "\n",
    "- **`torch.utils.data.Dataset, DataLoader`**  \n",
    "  `Dataset` is subclassed to create our custom `VideoFrameDataset`. The `DataLoader` wraps this dataset with batch processing, shuffling, and provides an iterator for training.\n",
    "\n",
    "### ğŸ–¼ï¸ Computer Vision & Image Processing\n",
    "\n",
    "- **`PIL.Image`**  \n",
    "  Used to convert between NumPy arrays and PIL images for compatibility with torchvision transforms.\n",
    "\n",
    "- **`cv2` (OpenCV)**  \n",
    "  Handles video operations (`VideoCapture`) to read and extract frames from video files, and image color space conversion (`cvtColor`).\n",
    "\n",
    "- **`torchvision.models`**  \n",
    "  Used to load a pre-trained `mobilenet_v2` as our base feature extractor, which we then modify for our specific classification task.\n",
    "\n",
    "- **`torchvision.models.mobilenetv2.MobileNet_V2_Weights`**\n",
    "  Provides pre-trained weight configurations for MobileNetV2 from PyTorch's model zoo. Used to initialize our model\n",
    "\n",
    "- **`torchvision.transforms`**  \n",
    "  Builds image transformation pipelines for:  \n",
    "  - Standard preprocessing: resize, crop, normalization  \n",
    "  - Data augmentation: random flips, rotations, color jitter, perspective changes\n",
    "\n",
    "### ğŸ“ Data Handling & Utilities\n",
    "\n",
    "- **`os`**  \n",
    "  Checks file existence (`os.path.exists()`) and constructs file paths (`os.path.join()`).\n",
    "\n",
    "- **`glob`**  \n",
    "  Finds image files with specific extensions (`.jpg`, `.jpeg`, `.png`) within directories.\n",
    "\n",
    "- **`random`**  \n",
    "  Samples subsets of data when we have too many files (`random.sample()`), helping maintain dataset balance.\n",
    "\n",
    "- **`numpy as np`**  \n",
    "  Used to transform images to NumPy arrays.\n",
    "\n",
    "### ğŸ“Š Visualization & Progress Tracking\n",
    "\n",
    "- **`matplotlib.pyplot as plt`**  \n",
    "  Creates and saves training visualization plots with loss and accuracy metrics after training completes.\n",
    "\n",
    "- **`tqdm`**  \n",
    "  Wraps the training loop to provide a progress bar with real-time metrics (loss and accuracy) during model training.\n",
    "\n",
    "These libraries together form a complete pipeline for processing video data, extracting frames, building and training a deep learning model for multi-class classification, and visualizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d0aba-b847-4435-bdfc-318962f346d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Core PyTorch Components\n",
    "# ======================\n",
    "import torch               # Base library for tensors, GPU ops, and autograd\n",
    "import torch.nn as nn      # Neural network layers (Linear, Conv2d, etc.)\n",
    "import torch.optim as optim  # Optimizer (Adam) for training\n",
    "from torch.utils.data import Dataset, DataLoader  # Custom datasets + efficient batching\n",
    "\n",
    "# ======================\n",
    "# Computer Vision\n",
    "# ======================\n",
    "import cv2                        # Video capture and frame processing\n",
    "from PIL import Image             # Image loading and conversion\n",
    "from torchvision import models    # Pretrained models (MobileNetV2)\n",
    "from torchvision.models.mobilenetv2 import ( # MobileNetV2-specific:\n",
    "    MobileNet_V2_Weights                     # - Pretrained weight configurations\n",
    ")\n",
    "from torchvision import transforms  # Image preprocessing/augmentations\n",
    "\n",
    "# ======================\n",
    "# Data Pipeline\n",
    "# ======================\n",
    "import os                        # File path operations\n",
    "from glob import glob            # Pattern matching for image/video files\n",
    "import random                    # Shuffling datasets and sampling\n",
    "import numpy as np               # Used to convert images to NumPy arrays\n",
    "\n",
    "# ======================\n",
    "# Training Utilities\n",
    "# ======================\n",
    "import matplotlib.pyplot as plt  # Plotting loss/accuracy curves\n",
    "from tqdm import tqdm            # Progress bars for training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd8046-b414-4872-9661-533e52733b24",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de83e6-f723-4e42-bd24-bcd550690961",
   "metadata": {},
   "source": [
    "## MobileNetV2 Transfer Learning Architecture Explanation ğŸš€\n",
    "\n",
    "### Project Goal: 4-Way Classification System ğŸ¯\n",
    "\n",
    "This implementation adapts a pre-trained MobileNetV2 model for a specific 4-class recognition task using transfer learning principles. The model is designed to classify webcam frames into:\n",
    "\n",
    "1. **Pet** â€“ A specific cat ğŸ±  \n",
    "2. **Owner**ğŸ‘¨  \n",
    "3. **Other Person** â€“ Any human who is not the ownerğŸ§  \n",
    "4. **None/Background** â€“ Empty frames with no people or pets ğŸš«\n",
    "\n",
    "### Base Model: MobileNetV2 ğŸ“±\n",
    "\n",
    "```python\n",
    "model = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1).to(device)\n",
    "```\n",
    "\n",
    "#### Why MobileNetV2? ğŸ¤”\n",
    "\n",
    "- **Efficiency** âš¡: MobileNetV2 is designed specifically for mobile and edge devices, making it computationally efficient while maintaining good accuracy\n",
    "- **Depthwise Separable Convolutions** ğŸ§©: Uses a factorized form of standard convolutions that drastically reduces parameter count and computational cost\n",
    "- **Inverted Residuals** ğŸ”„: Unlike traditional residual blocks, MobileNetV2 uses inverted residuals with linear bottlenecks, which help preserve information flow while keeping the model lightweight\n",
    "- **Pre-trained Knowledge** ğŸ§ : ImageNet pre-training provides the model with powerful feature extraction capabilities that can transfer well to new domains\n",
    "- **Size-Performance Tradeoff** âš–ï¸: Offers an excellent balance between model size (~14M parameters) and performance for real-time or resource-constrained applications\n",
    "\n",
    "### Transfer Learning Approach ğŸ”„\n",
    "\n",
    "```python\n",
    "# Freeze the feature extraction layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "```\n",
    "\n",
    "#### Benefits of Feature Freezing â„ï¸\n",
    "\n",
    "- **Training Efficiency** ğŸï¸: By freezing the convolutional backbone, we dramatically reduce the number of trainable parameters (from millions to thousands)\n",
    "- **Prevents Overfitting** ğŸ›¡ï¸: With limited training data, updating only the classifier prevents the model from overfitting to peculiarities in our small dataset\n",
    "- **Knowledge Preservation** ğŸ“š: Retains the robust feature extraction capabilities learned from ImageNet's diverse 1.2+ million images\n",
    "- **Faster Convergence** ğŸ: The classifier can adapt to the new task much more quickly when starting from well-formed features\n",
    "\n",
    "### Custom Classifier Head ğŸ‘‘\n",
    "\n",
    "```python\n",
    "model.classifier[1] = nn.Sequential(\n",
    "    nn.Linear(model.classifier[1].in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(256, 4)\n",
    ").to(device)\n",
    "```\n",
    "\n",
    "#### Classifier Design Choices ğŸ§©\n",
    "\n",
    "- **Intermediate Hidden Layer (256 neurons)** ğŸ§¬:\n",
    "  - Provides greater representational capacity than a direct mapping\n",
    "  - Allows the model to learn more complex decision boundaries between classes\n",
    "  - 256 neurons balances expressivity and computational efficiency\n",
    "  - Particularly helpful for distinguishing between similar human faces (owner vs. other person) ğŸ‘¥\n",
    "\n",
    "- **ReLU Activation** âš¡:\n",
    "  - Introduces non-linearity to capture complex relationships\n",
    "  - Mitigates the vanishing gradient problem with its non-saturating form\n",
    "  - Computationally efficient compared to tanh or sigmoid\n",
    "\n",
    "- **Dropout (0.2)** ğŸ­:\n",
    "  - Implements regularization by randomly deactivating 20% of neurons during training\n",
    "  - Prevents co-adaptation of neurons (neurons becoming too dependent on each other)\n",
    "  - Forces the network to learn redundant representations, improving generalization\n",
    "  - Rate of 0.2 is conservative, providing regularization while preserving most information flow\n",
    "  - Particularly important for this task since the dataset likely contains many similar frames of the same subjects\n",
    "\n",
    "- **Output Layer (4 neurons)** ğŸ¬:\n",
    "  - One neuron per class (pet, owner, other person, background)\n",
    "  - Used with CrossEntropyLoss which applies softmax internally to produce probabilities\n",
    "\n",
    "### Weight Initialization Strategy ğŸ²\n",
    "\n",
    "```python\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "```\n",
    "\n",
    "#### Xavier/Glorot Initialization Benefits âœ¨\n",
    "\n",
    "- **Variance Control** ğŸ“Š: Maintains the variance of activations and gradients across layers\n",
    "- **Prevents Signal Vanishing/Exploding** ğŸ’¥: Scaling weights based on the layer size helps signal propagate effectively\n",
    "- **Uniform Distribution** ğŸ“ˆ: Draws weights from a uniform distribution within a carefully calculated range\n",
    "- **Faster Convergence** ğŸš€: Well-initialized weights allow the model to reach optimal regions more quickly\n",
    "- **Zero Bias Initialization** 0ï¸âƒ£: Starting biases at zero is a standard practice that works well with ReLU when batch normalization isn't used\n",
    "\n",
    "### Model Deployment ğŸš€\n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "```\n",
    "\n",
    "#### Hardware Acceleration ğŸ’»\n",
    "\n",
    "- **Dynamic Device Selection** ğŸ”: Code automatically selects GPU if available, falling back to CPU if necessary\n",
    "- **Memory Management** ğŸ’¾: Moving the model to the appropriate device ensures efficient memory usage\n",
    "- **Computation Speed** âš¡: Running on GPU can offer 10-100x speedup for neural network operations\n",
    "\n",
    "### Architecture Summary ğŸ“\n",
    "\n",
    "This architecture exemplifies modern transfer learning best practices by:\n",
    "\n",
    "1. Leveraging a pre-trained, efficient CNN architecture ğŸ—ï¸\n",
    "2. Freezing feature extraction layers to preserve learned representations â„ï¸\n",
    "3. Implementing a purpose-built classifier for the specific task ğŸ¯\n",
    "4. Using appropriate regularization techniques to prevent overfitting ğŸ›¡ï¸\n",
    "5. Applying proven weight initialization strategies for faster convergence ğŸ\n",
    "\n",
    "The resulting model balances computational efficiency with classification performance, making it suitable for deployment in resource-constrained environments while still maintaining high accuracy for this pet and person recognition task. ğŸ¤–ğŸ‘\n",
    "\n",
    "### Application-Specific Advantages ğŸŒŸ\n",
    "\n",
    "For this specific pet/owner recognition task:\n",
    "\n",
    "1. **Fine-Grained Recognition** ğŸ”: The model can learn subtle differences between a specific cat and other animals, or between the owner and other people\n",
    "3. **Real-time Processing** â±ï¸: MobileNetV2's efficiency enables real-time classification on webcam streams\n",
    "4. **Low Resource Requirements** ğŸ’ª: The architecture can run on modest hardware like laptops without dedicated GPUs\n",
    "5. **Quick Training** ğŸï¸: By using transfer learning, the model can be trained with relatively few examples of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69abfb33-9ce8-4b30-b3bf-d0047046fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Model Definition\n",
    "# ======================\n",
    "# Set computation device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained MobileNetV2 with ImageNet weights\n",
    "# MobileNetV2 is chosen for its efficiency, performance, and lightweight nature\n",
    "model = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1).to(device)\n",
    "\n",
    "# Freeze the feature extraction layers to preserve pre-trained knowledge\n",
    "# This implements transfer learning - we only train the classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier with our custom head for 4-class classification\n",
    "# Architecture: Input features â†’ 256 neurons â†’ ReLU â†’ Dropout â†’ 4 output classes\n",
    "model.classifier[1] = nn.Sequential(\n",
    "    nn.Linear(model.classifier[1].in_features, 256),  # Hidden layer with 256 neurons\n",
    "    nn.ReLU(),                                        # Non-linearity\n",
    "    nn.Dropout(0.2),                                  # Regularization to prevent overfitting\n",
    "    nn.Linear(256, 4)                                 # Output layer for our 4 classes\n",
    ").to(device)\n",
    "\n",
    "# Define weight initialization function for better convergence\n",
    "# Xavier/Glorot initialization helps control variance of activations across layers\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)  # Uniform distribution within proper bounds\n",
    "        nn.init.zeros_(m.bias)             # Initialize biases to zero\n",
    "\n",
    "# Apply weight initialization to our classifier only\n",
    "model.classifier[1].apply(init_weights)\n",
    "\n",
    "# Verify model setup\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(\"Classifier structure:\")\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d361f0a-a50a-4360-9844-50151d65ad6c",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029fb06-3a55-45ab-91a3-02b06e8a6a00",
   "metadata": {},
   "source": [
    "## Training Configuration: Optimizer and Loss Function ğŸ› ï¸\n",
    "\n",
    "### Optimizer: Adam with Selective Training ğŸ¯\n",
    "\n",
    "```python\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(trainable_params, lr=0.001, weight_decay=1e-5)\n",
    "```\n",
    "\n",
    "#### Parameter Selection Strategy ğŸ”\n",
    "\n",
    "We use `filter(lambda p: p.requires_grad, model.parameters())` to select only trainable parameters. This is critical for our transfer learning approach:\n",
    "\n",
    "- **Efficiency Boost** âš¡: By optimizing only the classifier parameters ( ~few thousand) instead of the entire model ( ~14 million), we drastically reduce computation time and memory requirements\n",
    "- **Focused Learning** ğŸ”­: Updates are restricted to the task-specific components (the classifier), leaving the pre-trained feature extractor untouched\n",
    "- **Lambda Function Elegance** ğŸ’»: The lambda function creates a clean, one-line filter that selects parameters where `requires_grad=True`\n",
    "\n",
    "#### Why Adam? ğŸ¤”\n",
    "\n",
    "Adam (Adaptive Moment Estimation) combines the benefits of two other extensions of stochastic gradient descent:\n",
    "\n",
    "- **Adaptive Learning Rates** ğŸ“Š: Automatically adjusts learning rates for each parameter based on historical gradients\n",
    "- **Momentum** ğŸï¸: Accelerates convergence by adding a fraction of the previous update direction\n",
    "- **RMSProp Integration** ğŸ“‰: Adapts learning rates based on the average of recent magnitudes of gradients\n",
    "- **Sparse Gradient Handling** ğŸŒµ: Performs well even when gradients are sparse or noisy\n",
    "\n",
    "#### Hyperparameter Choices ğŸ›ï¸\n",
    "\n",
    "- **Learning Rate (5e-5)** ğŸ: \n",
    "  - A smaller learning rate typically used for fine-tuning pre-trained models.\n",
    "  - Helps ensure that the model updates its weights gradually, preventing large, unstable changes.\n",
    "  - Useful when working with pre-trained weights, as it allows the model to make subtle adjustments without drastically altering the learned features.\n",
    "  - Small enough to avoid overshooting the optimal solution, promoting stable convergence, especially in later training stages.\n",
    "\n",
    "- **Weight Decay (1e-5)** ğŸŒ±: \n",
    "  - Implements L2 regularization by penalizing large weights\n",
    "  - Helps prevent overfitting by encouraging the model to use smaller weights\n",
    "  - Value of 1e-5 is conservative, providing gentle regularization\n",
    "  - Particularly valuable for our problem where limited training data could lead to overfitting\n",
    "\n",
    "### Loss Function: CrossEntropyLoss with Label Smoothing ğŸ“Š\n",
    "\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "```\n",
    "\n",
    "#### Why CrossEntropyLoss? ğŸ¯\n",
    "\n",
    "CrossEntropyLoss is the standard choice for multi-class classification problems:\n",
    "\n",
    "- **Softmax Integration** ğŸ”„: Internally applies softmax to convert logits to probabilities\n",
    "- **Class Probability** ğŸ“Š: Measures the difference between predicted probability distribution and the target distribution\n",
    "- **Numerical Stability** ğŸ›¡ï¸: Implements log-softmax and negative log-likelihood in a numerically stable way\n",
    "- **Single-Label Focus** ğŸ·ï¸: Optimized for problems where each example belongs to exactly one class\n",
    "\n",
    "#### Label Smoothing (0.1) ğŸ¥¤\n",
    "\n",
    "Label smoothing is a regularization technique that modifies the target distribution:\n",
    "\n",
    "- **Target Softening** â˜ï¸: Instead of using hard labels (0,0,1,0), it creates soft targets (0.03,0.03,0.9,0.03)\n",
    "- **Confidence Penalty** ğŸ“‰: Discourages the model from becoming too confident in its predictions\n",
    "- **Overfitting Prevention** ğŸ›¡ï¸: Makes the model less likely to memorize training data noise or errors\n",
    "- **Class Imbalance Handling** âš–ï¸: Particularly valuable for our use case with potential class imbalance (may have more background frames than pet frames)\n",
    "- **Value Selection (0.1)** ğŸšï¸: \n",
    "  - 0.1 is a moderate smoothing value that provides regularization benefits\n",
    "  - High enough to prevent overconfidence\n",
    "  - Low enough to maintain class separation\n",
    "  \n",
    "### Combined Effect on Training Dynamics ğŸ”„\n",
    "\n",
    "Together, these choices create a training configuration that:\n",
    "\n",
    "1. **Focuses computational effort** on adapting the classifier to our specific classes\n",
    "2. **Adapts learning dynamically** based on gradient behavior during training\n",
    "3. **Regularizes from multiple angles** (weight decay and label smoothing) to prevent overfitting\n",
    "4. **Improves generalization** especially in the face of class imbalance or limited training data\n",
    "5. **Accelerates convergence** compared to standard SGD or simpler optimizers\n",
    "\n",
    "This configuration represents modern deep learning best practices for transfer learning on classification tasks with limited, potentially imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba882d62-0c1f-44c9-ac41-7752c5b79b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer (only train classifier parameters)\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())  \n",
    "# Filter out the parameters that require gradients (only the custom classifier will be trained)\n",
    "# This is efficient because we freeze the feature extraction layers, so we don't need to train them again.\n",
    "optimizer = optim.Adam(trainable_params, lr=5e-5, weight_decay=1e-5)  \n",
    "# Adam optimizer is chosen for adaptive learning rates, which helps to converge faster and more efficiently\n",
    "# lr=5e-5 (=0.00005): A learning rate of 5e-5 is a commonly used starting point for fine-tuning pre-trained models.\n",
    "# It allows the model to adjust the new classifier layers without drastically altering the pre-trained weights.\n",
    "# It's generally small enough to avoid overfitting but large enough to enable the classifier to learn efficiently.\n",
    "# weight_decay=1e-5: L2 regularization helps prevent overfitting by penalizing large weights (helps generalization).\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  \n",
    "# CrossEntropyLoss is commonly used for multi-class classification tasks, as it combines softmax and negative log-likelihood loss\n",
    "# label_smoothing=0.1: This technique reduces the confidence of the model when predicting the target class, \n",
    "# making it less likely to overfit on noisy or incorrect labels and improving generalization, especially for imbalanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4363382a-8f8a-4992-91ad-1ad931ad1f6b",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ed4c8-db25-479a-80a9-eb33265be1e9",
   "metadata": {},
   "source": [
    "## ğŸ¥ğŸ“¸ VideoImageDataset Class: A Comprehensive Overview\n",
    "\n",
    "### ğŸ—ï¸ Introduction\n",
    "\n",
    "The `VideoImageDataset` class is a custom PyTorch `Dataset` implementation designed for handling **mixed media (videos ğŸ¥ and images ğŸ“¸)** in machine learning classification tasks. Although initially designed for person/pet ğŸ• classification, it is general enough to apply to various domains like surveillance, smart home applications, or content moderation.\n",
    "\n",
    "This class demonstrates advanced data engineering strategies to improve efficiency, flexibility, and robustness during model training.\n",
    "\n",
    "### âš™ï¸ Core Functionality\n",
    "\n",
    "This class addresses several key challenges in multimedia datasets:\n",
    "\n",
    "1. **Mixed Media Handling** ğŸ”„  \n",
    "   Combines both video frames and image files into a unified dataset. No need to treat them separately during model training.\n",
    "\n",
    "2. **Memory Efficiency** ğŸ§   \n",
    "   Lazy loads frames only when needed and uses an internal cache to avoid repeated expensive disk I/O.\n",
    "\n",
    "3. **Data Augmentation** ğŸ¨  \n",
    "   Augmentation strategies can be applied consistently across both images and video frames, which improves generalization and robustness.\n",
    "\n",
    "4. **Robust Error Handling** ğŸ›¡ï¸  \n",
    "   Can skip unreadable or corrupted files without interrupting training â€” useful when datasets come from diverse or noisy sources.\n",
    "\n",
    "### ğŸ›ï¸ Class Architecture\n",
    "\n",
    "#### ğŸ”§ Initialization\n",
    "\n",
    "Constructor parameters:\n",
    "\n",
    "- `data_sources`: Dict mapping class names (e.g., \"pet\", \"owner\") to file paths (video/image)\n",
    "- `label_mapping`: Maps each class name to a unique integer (used as label)\n",
    "- `transform`: Optional torchvision transform pipeline (e.g., Resize, Normalize)\n",
    "- `augment`: Enables or disables augmentation (`True/False`)\n",
    "- `num_augments`: Number of augmentations to generate per frame/image (helps simulate diversity)\n",
    "- `max_frames_per_video`: Limits how many frames are sampled per video\n",
    "- `frame_interval`: Controls temporal sampling by picking every nth frame\n",
    "- `cache_size`: Max number of decoded frames to keep in memory\n",
    "\n",
    "**Initialization flow:**\n",
    "\n",
    "1. ğŸ“¥ Stores all input arguments  \n",
    "2. ğŸ§  Initializes internal memory-efficient cache (FIFO by default)  \n",
    "3. ğŸ—‚ï¸ Builds an internal index of data with paths, types, labels, and metadata  \n",
    "4. ğŸ§ª Validates all media files (image size, video readability)  \n",
    "5. ğŸ“Š Reports useful statistics (class distribution, number of samples, etc.)\n",
    "\n",
    "### ğŸ“‚ Data Indexing\n",
    "\n",
    "#### `_index_all_data()`\n",
    "\n",
    "This core method:\n",
    "\n",
    "- Iterates over each class label in `data_sources`\n",
    "- Differentiates between video and image paths\n",
    "- Indexes each file with a type tag (image or video)\n",
    "- Validates existence/readability early to avoid runtime errors\n",
    "- Collects metadata: dimensions, number of frames, FPS, etc.\n",
    "\n",
    "#### `_index_image(path)`\n",
    "\n",
    "- âœ”ï¸ Confirms file exists and is readable\n",
    "- ğŸ–¼ï¸ Loads dimensions and filters out corrupt or extremely small files\n",
    "- ğŸ§¾ Records metadata to avoid reloading this information later\n",
    "\n",
    "#### `_index_video(path)`\n",
    "\n",
    "- âœ”ï¸ Uses `cv2.VideoCapture` to extract video info\n",
    "- â³ Computes duration, total frames, and ensures file is valid\n",
    "- ğŸ¯ Selects which frames to use (based on the sampling strategy)\n",
    "- ğŸ“Œ Saves metadata for lazy loading later\n",
    "\n",
    "### ğŸ¯ Data Sampling Strategies\n",
    "\n",
    "Three customizable sampling strategies for videos:\n",
    "\n",
    "1. **Uniform Sampling** â±ï¸  \n",
    "   Evenly spaced frames (default) â€” useful for general-purpose training.\n",
    "\n",
    "2. **Front-Weighted Sampling** â©  \n",
    "   Biases frame selection toward the beginning â€” useful for intro-heavy clips.\n",
    "\n",
    "3. **Random Sampling** ğŸ²  \n",
    "   Randomly selects a subset of frames â€” adds stochasticity to training.\n",
    "\n",
    "Can be set via a sampling policy attribute or parameter.\n",
    "\n",
    "### ğŸ“¤ Data Loading\n",
    "\n",
    "#### `__getitem__(index)`\n",
    "\n",
    "- ğŸ§® Computes actual sample from the internal index  \n",
    "- ğŸ” Loads the specific frame or image  \n",
    "- ğŸ§  Checks if it's already cached; if not, reads and caches it  \n",
    "- ğŸ¨ Applies augmentation if enabled  \n",
    "- ğŸ§ª Applies base transform (resizing, normalization, etc.)  \n",
    "- ğŸ·ï¸ Returns a `(tensor_image, label)` pair\n",
    "\n",
    "### ğŸ–¼ï¸ Frame Loading Logic\n",
    "\n",
    "#### `_load_video_frame(video_path, frame_index)`\n",
    "\n",
    "- Checks frame cache  \n",
    "- If missing, reads frame with OpenCV and adds to cache  \n",
    "- Converts BGR â†’ RGB and normalizes  \n",
    "- Handles any reading or decoding errors gracefully\n",
    "\n",
    "#### `_load_image(image_path)`\n",
    "\n",
    "- Also supports caching  \n",
    "- Performs format conversion and integrity check  \n",
    "- Skips unreadable files and optionally returns dummy data\n",
    "\n",
    "### ğŸ¨ Data Augmentation\n",
    "\n",
    "Augmentations can simulate realistic conditions, such as:\n",
    "\n",
    "1. **Color Augmentations** ğŸŒˆ  \n",
    "   - Brightness/contrast shifts  \n",
    "   - Hue/saturation jitter  \n",
    "\n",
    "2. **Geometric Augmentations** ğŸ“  \n",
    "   - Random crops, flips, rotations  \n",
    "\n",
    "3. **Mixed Augmentations** ğŸ­  \n",
    "   - Combine both color and spatial transforms  \n",
    "\n",
    "These can be toggled or extended with `torchvision.transforms`.\n",
    "\n",
    "### ğŸ’¾ Cache Management\n",
    "\n",
    "- ğŸ—ƒï¸ Internal dict stores decoded frames\n",
    "- ğŸ” Uses FIFO eviction when full (to save memory)\n",
    "- ğŸ“‰ Tracks hit/miss rate for performance diagnostics\n",
    "- ğŸ› ï¸ Makes repeated accesses much faster (e.g., DataLoader with `num_workers > 0`)\n",
    "\n",
    "### ğŸ§  Design Choices Explained\n",
    "\n",
    "#### â³ Lazy Loading  \n",
    "Only loads what you use, when you use it. Saves RAM and GPU memory.\n",
    "\n",
    "#### ğŸ’¾ Why Use a Cache?  \n",
    "Reading from disk is slow â€” cache helps especially when using augmentation or repeated data access.\n",
    "\n",
    "#### ğŸ¨ Multiple Augmentations  \n",
    "Improves model robustness and performance across unseen scenarios or lighting conditions.\n",
    "\n",
    "#### ğŸ“Š Statistics Tracking  \n",
    "Important for debugging, understanding class balance, and verifying dataset health.\n",
    "\n",
    "#### ğŸ›¡ï¸ Robust Error Handling  \n",
    "Doesnâ€™t crash on corrupted data â€” reports it and optionally replaces with dummy image.\n",
    "\n",
    "### ğŸ† Practical Applications  \n",
    "\n",
    "Great for:  \n",
    "1. ğŸ• Pet/Owner Recognition  \n",
    "2. ğŸ”’ Security Systems  \n",
    "3. ğŸ  Smart Home Applications  \n",
    "\n",
    "### ğŸ¬ Conclusion  \n",
    "\n",
    "The VideoImageDataset class demonstrates advanced PyTorch techniques for:  \n",
    "- ğŸ§  Memory efficiency  \n",
    "- âš¡ Performance  \n",
    "- ğŸ”„ Flexibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7888ee1-ff96-4f93-b548-823b7720630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoImageDataset(Dataset):\n",
    "    def __init__(self, data_sources, label_mapping, transform=None, \n",
    "             augment=False, num_augments=3, max_frames_per_video=300,\n",
    "             frame_interval=1, cache_size=500):\n",
    "        \"\"\"\n",
    "        Initialize a dataset which can handle both video and image data for person/pet classification.\n",
    "        \n",
    "        Args:\n",
    "            data_sources (dict): Dictionary containing paths to video/image files organized by class\n",
    "                               Format: {'owner': {'video_paths': [...], 'image_dirs': [...]}, 'pet': {...}}\n",
    "            label_mapping (dict): Mapping from class names to integer labels \n",
    "                                 (e.g., {'owner': 0, 'pet': 1})\n",
    "            transform (callable, optional): Torchvision transforms for image preprocessing\n",
    "            augment (bool): Whether to apply data augmentation\n",
    "            num_augments (int): Number of augmented versions to create per original frame\n",
    "            max_frames_per_video (int): Maximum frames to sample from each video\n",
    "            frame_interval (int): Sample every nth frame from videos\n",
    "            cache_size (int): Maximum number of frames to keep in memory cache\n",
    "        \"\"\"\n",
    "        \n",
    "        # Store initialization parameters\n",
    "        self.data_sources = data_sources        # Source paths organized by class\n",
    "        self.label_mapping = label_mapping      # Class name to label index mapping\n",
    "        self.transform = transform              # Image preprocessing pipeline\n",
    "        self.augment = augment                  # Data augmentation flag\n",
    "        self.num_augments = num_augments        # Number of augmentations per frame\n",
    "        self.max_frames_per_video = max_frames_per_video  # Frame sampling limit\n",
    "        self.frame_interval = frame_interval    # Interval between sampled frames\n",
    "        \n",
    "        # Setup caching system\n",
    "        self.use_cache = cache_size > 0\n",
    "        if self.use_cache:\n",
    "            self.cache_limit = cache_size\n",
    "            self.frame_cache = {}\n",
    "            self.cache_stats = {'hits': 0, 'misses': 0}\n",
    "        \n",
    "        # Data storage structures\n",
    "        self.frame_sources = []  # Stores tuples of (source_type, path, frame_idx, label, metadata)\n",
    "        self.class_counts = {name: 0 for name in label_mapping}  # Tracks samples per class\n",
    "        self.video_metadata = {}  # Store video properties\n",
    "        self.image_metadata = {}  # Store image properties\n",
    "        \n",
    "        # Track the source files for statistics\n",
    "        self.source_tracking = {class_name: {'video_paths': set(), 'image_paths': set()} \n",
    "                               for class_name in label_mapping}\n",
    "        \n",
    "        # Build the dataset by indexing all available data\n",
    "        self._index_all_data()\n",
    "        \n",
    "        # Validate that we found some data\n",
    "        if len(self.frame_sources) == 0:\n",
    "            raise ValueError(\"No valid frames were indexed. Please check your input paths.\")\n",
    "        \n",
    "        # Calculate total samples including augmentations\n",
    "        total_samples = len(self.frame_sources)\n",
    "        if self.augment:\n",
    "            total_samples *= (1 + self.num_augments)\n",
    "        \n",
    "        # Print dataset statistics\n",
    "        print(f\"\\nDataset created with {total_samples} samples\")\n",
    "        print(\"Class distribution:\")\n",
    "        for name, idx in label_mapping.items():\n",
    "            # Calculate effective count including augmentations\n",
    "            count = self.class_counts[name] * (1 + self.num_augments if self.augment else 1)\n",
    "            print(f\"- {name}: {count} samples ({count/total_samples:.1%})\")\n",
    "\n",
    "    def _index_all_data(self):\n",
    "        \"\"\"\n",
    "        Master method to index all data types including videos and image directories.\n",
    "        This processes each class defined in data_sources, including owner, pet, other people, and background.\n",
    "        Progress is reported to console during indexing operations.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STARTING DATA INDEXING\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Track statistics for final report\n",
    "        indexed_counts = {class_name: {'images': 0, 'videos': 0} for class_name in self.label_mapping}\n",
    "        \n",
    "        # Process each class in the data sources\n",
    "        for class_name, sources in self.data_sources.items():\n",
    "            if class_name not in self.label_mapping:\n",
    "                print(f\"Warning: Class '{class_name}' not found in label_mapping, skipping\")\n",
    "                continue\n",
    "                \n",
    "            class_label = self.label_mapping[class_name]\n",
    "            \n",
    "            print(f\"\\n{'-'*20}\")\n",
    "            print(f\"INDEXING {class_name.upper()} DATA\")\n",
    "            print(f\"{'-'*20}\")\n",
    "            \n",
    "            # Process image directories first\n",
    "            if 'image_dirs' in sources and sources['image_dirs']:\n",
    "                image_dirs = sources['image_dirs'] if isinstance(sources['image_dirs'], list) else [sources['image_dirs']]\n",
    "                \n",
    "                for directory in image_dirs:\n",
    "                    if os.path.isdir(directory):\n",
    "                        print(f\"\\nIndexing {class_name} images from: {directory}\")\n",
    "                        image_paths = self._get_image_paths(directory)\n",
    "                        \n",
    "                        if image_paths:\n",
    "                            # Sample a reasonable number of images if there are too many\n",
    "                            sample_size = min(500, len(image_paths))\n",
    "                            if len(image_paths) > sample_size:\n",
    "                                print(f\"Found {len(image_paths)} images, sampling {sample_size}...\")\n",
    "                                sampled_paths = random.sample(image_paths, sample_size)\n",
    "                            else:\n",
    "                                print(f\"Found {len(image_paths)} images, processing all...\")\n",
    "                                sampled_paths = image_paths\n",
    "                            \n",
    "                            # Process the images\n",
    "                            successful_count = 0\n",
    "                            for i, img_path in enumerate(sampled_paths, 1):\n",
    "                                if i % 50 == 0:  # Progress update every 50 images\n",
    "                                    print(f\"Indexed {i}/{len(sampled_paths)} {class_name} images\")\n",
    "                                if self._index_image(img_path, class_label, class_name):\n",
    "                                    indexed_counts[class_name]['images'] += 1\n",
    "                                    successful_count += 1\n",
    "                                    # Add to tracking for statistics\n",
    "                                    self.source_tracking[class_name]['image_paths'].add(img_path)\n",
    "                            \n",
    "                            print(f\"Successfully indexed {successful_count} {class_name} images\")\n",
    "                            print()\n",
    "                        else:\n",
    "                            print(f\"No valid images found in {directory}\")\n",
    "                    else:\n",
    "                        print(f\"WARNING: {class_name} image directory does not exist: {directory}\")\n",
    "            \n",
    "            # Then process videos if specified\n",
    "            if 'video_paths' in sources and sources['video_paths']:\n",
    "                video_paths = sources['video_paths'] if isinstance(sources['video_paths'], list) else [sources['video_paths']]\n",
    "                \n",
    "                for path in video_paths:\n",
    "                    if os.path.exists(path):\n",
    "                        print(f\"\\nIndexing {class_name} video: {path}\")\n",
    "                        frames_processed = self._index_video(path, class_label, class_name)\n",
    "                        if frames_processed > 0:\n",
    "                            indexed_counts[class_name]['videos'] += 1\n",
    "                            # Add to tracking for statistics\n",
    "                            self.source_tracking[class_name]['video_paths'].add(path)\n",
    "                        print(f\"Processed {frames_processed} frames from {class_name} video\")\n",
    "                    else:\n",
    "                        print(f\"WARNING: {class_name} video path does not exist: {path}\")\n",
    "        \n",
    "        # Print summary of indexed data\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"DATA INDEXING COMPLETE\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\nINDEXING SUMMARY:\")\n",
    "        for class_name in self.label_mapping:\n",
    "            counts = indexed_counts.get(class_name, {'images': 0, 'videos': 0})\n",
    "            print(f\"{class_name.capitalize()} data: {counts['images']} images, {counts['videos']} videos\")\n",
    "    \n",
    "    def _get_image_paths(self, directory):\n",
    "        \"\"\"\n",
    "        Get all image file paths from a directory.\n",
    "        Supports .jpg, .jpeg, and .png files.\n",
    "        \n",
    "        Args:\n",
    "            directory (str): Path to the directory containing images\n",
    "            \n",
    "        Returns:\n",
    "            list: List of image file paths\n",
    "        \"\"\"\n",
    "        return glob(os.path.join(directory, \"*.jpg\")) + \\\n",
    "               glob(os.path.join(directory, \"*.jpeg\")) + \\\n",
    "               glob(os.path.join(directory, \"*.png\"))\n",
    "\n",
    "    def _index_video(self, path, label, class_name):\n",
    "        \"\"\"\n",
    "        Index frames from a video file and store references for lazy loading.\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to the video file\n",
    "            label (int): Numeric label for classification\n",
    "            class_name (str): Class name for tracking statistics\n",
    "        \n",
    "        Returns:\n",
    "            int: Number of frames successfully indexed\n",
    "        \"\"\"\n",
    "        frames_indexed = 0\n",
    "        \n",
    "        try:\n",
    "            # Validate file exists\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Error: Video file not found - {path}\")\n",
    "                return frames_indexed\n",
    "            \n",
    "            # Open video capture\n",
    "            cap = cv2.VideoCapture(path)\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Error: Could not open video - {path}\")\n",
    "                return frames_indexed\n",
    "            \n",
    "            # Get video properties\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            duration = total_frames / fps if fps > 0 else 0\n",
    "            \n",
    "            if total_frames <= 0:\n",
    "                print(f\"Warning: Video has {total_frames} frames - {path}\")\n",
    "                cap.release()\n",
    "                return frames_indexed\n",
    "            \n",
    "            # Calculate exact number of frames to sample\n",
    "            target_frames = min(self.max_frames_per_video, total_frames)\n",
    "            \n",
    "            # Different sampling strategies based on video length\n",
    "            if total_frames <= target_frames:\n",
    "                # If video is short, take all frames with frame_interval\n",
    "                frame_indices = list(range(0, total_frames, self.frame_interval))\n",
    "            else:\n",
    "                # For longer videos, choose between:\n",
    "                # 1. Uniform sampling (default)\n",
    "                # 2. Front-weighted sampling (more frames from beginning)\n",
    "                # 3. Random sampling\n",
    "                sampling_strategy = getattr(self, 'sampling_strategy', 'uniform')\n",
    "                \n",
    "                if sampling_strategy == 'front_weighted':\n",
    "                    # Sample more from the beginning (useful for pet/owner videos)\n",
    "                    first_half = int(total_frames * 0.5)\n",
    "                    first_half_frames = int(target_frames * 0.7)\n",
    "                    second_half_frames = target_frames - first_half_frames\n",
    "                    \n",
    "                    step1 = max(self.frame_interval, first_half // first_half_frames)\n",
    "                    step2 = max(self.frame_interval, (total_frames - first_half) // second_half_frames)\n",
    "                    \n",
    "                    frame_indices = list(range(0, first_half, step1))[:first_half_frames]\n",
    "                    frame_indices += list(range(first_half, total_frames, step2))[:second_half_frames]\n",
    "                elif sampling_strategy == 'random':\n",
    "                    # Randomly sample frames\n",
    "                    frame_indices = sorted(random.sample(range(total_frames), target_frames))\n",
    "                else:  # uniform\n",
    "                    # Uniform sampling throughout video\n",
    "                    step = max(self.frame_interval, total_frames // target_frames)\n",
    "                    frame_indices = list(range(0, total_frames, step))[:target_frames]\n",
    "            \n",
    "            print(f\"Indexing {len(frame_indices)} frames from {total_frames} total frames ({duration:.1f}s)\")\n",
    "            \n",
    "            # Store frame references for lazy loading\n",
    "            video_info = {\n",
    "                'path': path,\n",
    "                'fps': fps,\n",
    "                'total_frames': total_frames,\n",
    "                'duration': duration,\n",
    "                'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "                'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            }\n",
    "            \n",
    "            for frame_idx in frame_indices:\n",
    "                self.frame_sources.append((\"video\", path, frame_idx, label, video_info))\n",
    "                frames_indexed += 1\n",
    "                self.class_counts[class_name] += 1\n",
    "            \n",
    "            # Add video metadata to tracking\n",
    "            self.video_metadata[path] = video_info\n",
    "            \n",
    "            cap.release()\n",
    "            print(f\"Successfully indexed {frames_indexed} frames from {path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error indexing video {path}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        return frames_indexed\n",
    "    \n",
    "    def _index_image(self, img_path, label, class_name):\n",
    "        \"\"\"\n",
    "        Index an image file and store reference for lazy loading.\n",
    "        \n",
    "        Args:\n",
    "            img_path (str): Path to the image file\n",
    "            label (int): Numeric label for classification\n",
    "            class_name (str): Class name for tracking statistics\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if image was successfully indexed, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate file exists\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Error: Image file not found - {img_path}\")\n",
    "                return False\n",
    "            \n",
    "            # Check if it's a valid image by reading dimensions\n",
    "            # This helps catch corrupt images early without loading full content\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    width, height = img.size\n",
    "                    image_info = {\n",
    "                        'path': img_path,\n",
    "                        'width': width,\n",
    "                        'height': height,\n",
    "                        'format': img.format,\n",
    "                        'mode': img.mode\n",
    "                    }\n",
    "                    \n",
    "                    # Skip images that are too small\n",
    "                    min_size = getattr(self, 'min_image_size', 32)\n",
    "                    if width < min_size or height < min_size:\n",
    "                        print(f\"Skipping small image ({width}x{height}): {img_path}\")\n",
    "                        return False\n",
    "            except Exception as img_error:\n",
    "                print(f\"Invalid image file {img_path}: {str(img_error)}\")\n",
    "                return False\n",
    "            \n",
    "            # Store reference to the image\n",
    "            self.frame_sources.append((\"image\", img_path, 0, label, image_info))\n",
    "            self.class_counts[class_name] += 1\n",
    "            \n",
    "            # Track image metadata\n",
    "            self.image_metadata[img_path] = image_info\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error indexing image {img_path}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total length of the dataset including augmentations.\n",
    "        \n",
    "        Returns:\n",
    "            int: Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        base_len = len(self.frame_sources)\n",
    "        if self.augment and base_len > 0:\n",
    "            return base_len * (1 + self.num_augments)\n",
    "        return base_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset with lazy loading and augmentation.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image_tensor, label_tensor)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calculate which frame and whether this is an augmented version\n",
    "            if self.augment:\n",
    "                base_idx = idx // (1 + self.num_augments)\n",
    "                aug_version = idx % (1 + self.num_augments)\n",
    "            else:\n",
    "                base_idx = idx\n",
    "                aug_version = 0\n",
    "            \n",
    "            # Validate index\n",
    "            if base_idx >= len(self.frame_sources):\n",
    "                raise IndexError(f\"Index {base_idx} out of range for dataset with {len(self.frame_sources)} items\")\n",
    "            \n",
    "            # Get the frame source info\n",
    "            source_info = self.frame_sources[base_idx]\n",
    "            if len(source_info) >= 5:\n",
    "                source_type, path, frame_idx, label, metadata = source_info\n",
    "            else:\n",
    "                source_type, path, frame_idx, label = source_info\n",
    "                metadata = {}\n",
    "            \n",
    "            # Load the frame on demand\n",
    "            if source_type == \"video\":\n",
    "                frame = self._load_video_frame(path, frame_idx)\n",
    "            else:  # image\n",
    "                frame = self._load_image(path)\n",
    "            \n",
    "            # Track cache hits/misses if using cache\n",
    "            if self.use_cache:\n",
    "                self._update_cache_stats(path, frame_idx)\n",
    "            \n",
    "            # Apply augmentation if needed (ensure we only augment when aug_version > 0)\n",
    "            if self.augment and aug_version > 0:\n",
    "                frame = self._augment_frame(frame, aug_version)\n",
    "            \n",
    "            # Apply standard transformation\n",
    "            if self.transform:\n",
    "                if not isinstance(frame, Image.Image):\n",
    "                    frame = Image.fromarray(frame)\n",
    "                frame = self.transform(frame)\n",
    "            \n",
    "            return frame, torch.tensor(label, dtype=torch.long)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting item {idx}: {str(e)}\")\n",
    "            # Log more details about the problematic item\n",
    "            if 'base_idx' in locals() and base_idx < len(self.frame_sources):\n",
    "                print(f\"Problematic item details: {self.frame_sources[base_idx]}\")\n",
    "            \n",
    "            # Return a default item instead of crashing\n",
    "            dummy_shape = getattr(self, 'input_shape', (3, 224, 224))\n",
    "            dummy_data = torch.zeros(dummy_shape)\n",
    "            dummy_label = 0\n",
    "            return dummy_data, torch.tensor(dummy_label, dtype=torch.long)\n",
    "    \n",
    "    def _load_video_frame(self, video_path, frame_idx):\n",
    "        \"\"\"\n",
    "        Load a specific frame from a video file with caching support.\n",
    "        \n",
    "        Args:\n",
    "            video_path (str): Path to the video file\n",
    "            frame_idx (int): Index of the frame to load\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: RGB image as numpy array\n",
    "        \"\"\"\n",
    "        # Check cache first if enabled\n",
    "        if self.use_cache:\n",
    "            cache_key = f\"{video_path}_{frame_idx}\"\n",
    "            if cache_key in self.frame_cache:\n",
    "                return self.frame_cache[cache_key]\n",
    "        \n",
    "        # Load frame from video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise IOError(f\"Failed to open video: {video_path}\")\n",
    "        \n",
    "        # Position video to the specific frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if not ret or frame is None:\n",
    "            raise ValueError(f\"Failed to load frame {frame_idx} from {video_path}\")\n",
    "        \n",
    "        # Convert from BGR to RGB color space\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Cache the frame if caching is enabled\n",
    "        if self.use_cache:\n",
    "            cache_key = f\"{video_path}_{frame_idx}\"\n",
    "            self.frame_cache[cache_key] = rgb_frame\n",
    "            \n",
    "            # Limit cache size if needed\n",
    "            if len(self.frame_cache) > self.cache_limit:\n",
    "                # Remove oldest entry (simple FIFO implementation)\n",
    "                oldest_key = next(iter(self.frame_cache))\n",
    "                del self.frame_cache[oldest_key]\n",
    "        \n",
    "        return rgb_frame\n",
    "    \n",
    "    def _load_image(self, img_path):\n",
    "        \"\"\"\n",
    "        Load an image file with caching support.\n",
    "        \n",
    "        Args:\n",
    "            img_path (str): Path to the image file\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: RGB image as numpy array\n",
    "        \"\"\"\n",
    "        # Check cache first if enabled\n",
    "        if self.use_cache:\n",
    "            if img_path in self.frame_cache:\n",
    "                return self.frame_cache[img_path]\n",
    "        \n",
    "        # Try loading with PIL first (better color handling)\n",
    "        try:\n",
    "            with Image.open(img_path) as pil_img:\n",
    "                # Convert to RGB if needed\n",
    "                if pil_img.mode != 'RGB':\n",
    "                    pil_img = pil_img.convert('RGB')\n",
    "                img = np.array(pil_img)\n",
    "        except Exception as pil_error:\n",
    "            # Fall back to OpenCV\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Failed to load image {img_path}\")\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Cache the image if caching is enabled\n",
    "        if self.use_cache:\n",
    "            self.frame_cache[img_path] = img\n",
    "            \n",
    "            # Limit cache size if needed\n",
    "            if len(self.frame_cache) > self.cache_limit:\n",
    "                # Remove oldest entry\n",
    "                oldest_key = next(iter(self.frame_cache))\n",
    "                del self.frame_cache[oldest_key]\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def _augment_frame(self, frame, aug_version=1):\n",
    "        \"\"\"\n",
    "        Apply data augmentation to an image.\n",
    "        \n",
    "        Args:\n",
    "            frame (numpy.ndarray): Input image as numpy array\n",
    "            aug_version (int): Version of augmentation to apply (allows for different augmentation sets)\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray or PIL.Image: Augmented image\n",
    "        \"\"\"\n",
    "        # Convert frame to PIL if it's numpy array\n",
    "        if not isinstance(frame, Image.Image):\n",
    "            frame_pil = Image.fromarray(frame)\n",
    "        else:\n",
    "            frame_pil = frame\n",
    "        \n",
    "        # Apply different augmentation strategies based on version\n",
    "        if aug_version % 3 == 1:\n",
    "            # Color augmentations\n",
    "            augmenter = transforms.Compose([\n",
    "                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
    "                transforms.RandomGrayscale(p=0.05),\n",
    "                transforms.RandomAutocontrast(p=0.2),\n",
    "            ])\n",
    "        elif aug_version % 3 == 2:\n",
    "            # Geometric augmentations\n",
    "            augmenter = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            ])\n",
    "        else:\n",
    "            # Mixed augmentations\n",
    "            augmenter = transforms.Compose([\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.08, 0.08)),\n",
    "            ])\n",
    "        \n",
    "        # Apply the augmentation\n",
    "        augmented = augmenter(frame_pil)\n",
    "        \n",
    "        # Return in the same format as input\n",
    "        if isinstance(frame, np.ndarray):\n",
    "            return np.array(augmented)\n",
    "        return augmented\n",
    "    \n",
    "    def _update_cache_stats(self, path, frame_idx):\n",
    "        \"\"\"\n",
    "        Track cache hit/miss statistics for performance monitoring.\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to the file\n",
    "            frame_idx (int): Frame index (0 for images)\n",
    "        \"\"\"\n",
    "        cache_key = f\"{path}_{frame_idx}\" if frame_idx > 0 else path\n",
    "        \n",
    "        if cache_key in self.frame_cache:\n",
    "            self.cache_stats['hits'] += 1\n",
    "        else:\n",
    "            self.cache_stats['misses'] += 1\n",
    "        \n",
    "        # Log cache stats occasionally\n",
    "        total = self.cache_stats['hits'] + self.cache_stats['misses']\n",
    "        if total % 1000 == 0:\n",
    "            hit_rate = self.cache_stats['hits'] / total * 100 if total > 0 else 0\n",
    "            print(f\"Cache stats: {hit_rate:.1f}% hit rate ({self.cache_stats['hits']}/{total})\")\n",
    "    \n",
    "    def get_source_statistics(self):\n",
    "        \"\"\"\n",
    "        Return statistics about the data sources in this dataset.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing counts of videos and images per class\n",
    "        \"\"\"\n",
    "        stats = {}\n",
    "        for class_name in self.label_mapping.keys():\n",
    "            stats[class_name] = {\n",
    "                'videos': len(self.source_tracking[class_name]['video_paths']),\n",
    "                'images': len(self.source_tracking[class_name]['image_paths']),\n",
    "                'frames': self.class_counts[class_name]\n",
    "            }\n",
    "                \n",
    "        return stats\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ======================\n",
    "    # Dataset Configuration\n",
    "    # ======================\n",
    "    \n",
    "    # Define class labels and their corresponding integer mappings\n",
    "    label_mapping = {\n",
    "        \"owner\": 0,       # Class 0: Owner \n",
    "        \"pet\": 1,         # Class 1: Pet\n",
    "        \"other\": 2,       # Class 2: Other people\n",
    "        \"background\": 3   # Class 3: Empty background/scenes\n",
    "    }\n",
    "    \n",
    "    # Data source configurations\n",
    "    data_sources = {\n",
    "        \"owner\": {\n",
    "            \"video_paths\": [\"../data/owner/owner.mp4\"],\n",
    "            \"image_dirs\": [\"../data/owner/images\"]\n",
    "        },\n",
    "        \"pet\": {\n",
    "            \"video_paths\": [\"../data/pet/pet.mp4\"],\n",
    "            \"image_dirs\": [\"../data/pet/images\"]\n",
    "        },\n",
    "        \"other\": {\n",
    "            \"video_paths\": [\"../data/other_people/other_people.mp4\"],\n",
    "            \"image_dirs\": [\"../data/other_people/images\"]\n",
    "        },\n",
    "        \"background\": {\n",
    "            \"video_paths\": [\"../data/background/background.mp4\"],\n",
    "            \"image_dirs\": [\"../data/background/images\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Image preprocessing pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),            # Resize shorter side to 256px\n",
    "        transforms.CenterCrop(224),        # Crop to 224x224 (standard for ImageNet models)\n",
    "        transforms.ToTensor(),             # Convert to PyTorch tensor\n",
    "        transforms.Normalize(              # Normalize with ImageNet stats\n",
    "            mean=[0.485, 0.456, 0.406],    # RGB mean values\n",
    "            std=[0.229, 0.224, 0.225]      # RGB standard deviations\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # ======================\n",
    "    # Dataset Creation\n",
    "    # ======================\n",
    "    try:\n",
    "        print(\"Creating dataset...\")\n",
    "        dataset = VideoImageDataset(\n",
    "            data_sources=data_sources,     # Combined video and image sources\n",
    "            label_mapping=label_mapping,   # Class mappings\n",
    "            transform=transform,           # Preprocessing\n",
    "            augment=True,                  # Enable data augmentation\n",
    "            num_augments=4,                # 4 augmented versions per frame\n",
    "            max_frames_per_video=1750,     # Limit frames per video\n",
    "            frame_interval=1,              # Sample every frame\n",
    "            cache_size=1000                # Cache size for processed frames\n",
    "        )\n",
    "        \n",
    "        # ======================\n",
    "        # DataLoader Setup\n",
    "        # ======================\n",
    "        # Start with fewer workers for testing/debugging\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=16,                 # Number of samples per batch\n",
    "            shuffle=True,                  # Shuffle data each epoch\n",
    "            num_workers=0,                 # Synchronous loading (safe for debugging)\n",
    "            pin_memory=torch.cuda.is_available()  # Faster GPU transfer if available\n",
    "        )\n",
    "        \n",
    "        # ======================\n",
    "        # Sanity Check\n",
    "        # ======================\n",
    "        print()\n",
    "        print(\"Testing dataloader...\")\n",
    "        class_counts = {i: 0 for i in range(len(label_mapping))}\n",
    "        \n",
    "        for i, (x, y) in enumerate(dataloader):\n",
    "            # x: batch of images (shape: [batch_size, 3, 224, 224])\n",
    "            # y: batch of labels (shape: [batch_size])\n",
    "            print(f\"Batch {i}: x shape = {x.shape}, y shape = {y.shape}\")\n",
    "            \n",
    "            # Count class distribution\n",
    "            for label in y.numpy():\n",
    "                class_counts[label] += 1\n",
    "            \n",
    "            # Only check first 3 batches for quick verification\n",
    "            if i == 2:\n",
    "                break\n",
    "\n",
    "        print()\n",
    "        print(\"Class distribution in sampled batches:\")\n",
    "        for class_name, class_idx in label_mapping.items():\n",
    "            print(f\"  {class_name}: {class_counts[class_idx]} samples\")\n",
    "\n",
    "        print()\n",
    "        print(\"Dataloader test successful!\")\n",
    "        \n",
    "        # Report dataset statistics\n",
    "        print()\n",
    "        print(f\"Total dataset size: {len(dataset)} samples\")\n",
    "        print(f\"Sources loaded: {dataset.get_source_statistics()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle any errors during dataset creation or loading\n",
    "        print(f\"Error during dataset setup: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940fee7e-1180-4e67-b739-123dfb0de7f3",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c00727-fb47-4b0a-953c-66fcbc7d0e5a",
   "metadata": {},
   "source": [
    "## ğŸ§  Model Training Loop\n",
    "\n",
    "Letâ€™s break down whatâ€™s *really* happening under the hood during training â€” from setup to plotting final results.\n",
    "\n",
    "### ğŸš€ Setup and Launch\n",
    "\n",
    "Before training kicks off:\n",
    "\n",
    "- **Model to Device** ğŸ“¦  \n",
    "  Send the model to GPU (`cuda`) or CPU using `model.to(device)` for efficient computation.\n",
    "\n",
    "- **Loss Function** ğŸ¯  \n",
    "  We're using `CrossEntropyLoss` â€“ perfect for multi-class classification tasks.\n",
    "\n",
    "- **Optimizer** âš™ï¸  \n",
    "  `Adam` is our optimizer of choice, with a customizable learning rate.\n",
    "\n",
    "- **Device Check** ğŸ’»  \n",
    "  Automatically detects if CUDA is available and switches device accordingly.\n",
    "\n",
    "### ğŸ” Epochs\n",
    "\n",
    "Training runs over **`num_epochs`** (default: `10`).  \n",
    "Each epoch = **1 full pass** through the training data.\n",
    "\n",
    "Multiple epochs help the model gradually improve its performance through weight updates.\n",
    "\n",
    "### ğŸ“Š Metrics Tracked\n",
    "\n",
    "We're collecting and visualizing:\n",
    "\n",
    "- **Loss** ğŸ“‰ â†’ Measures prediction error (lower is better).\n",
    "- **Accuracy** âœ… â†’ Percentage of correct predictions.\n",
    "\n",
    "These are stored in:\n",
    "\n",
    "```python\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "```\n",
    "\n",
    "### ğŸ”„ Inside Each Epoch\n",
    "\n",
    "Here's what happens **for each epoch**:\n",
    "\n",
    "1. **Set Model to Training Mode** ğŸ‹ï¸  \n",
    "   Activates layers like `Dropout`, `BatchNorm`, etc.  \n",
    "   ```python\n",
    "   model.train()\n",
    "   ```\n",
    "\n",
    "2. **TQDM Progress Bar** â³  \n",
    "   Real-time batch tracking using:  \n",
    "   ```python\n",
    "   loop = tqdm(train_loader)\n",
    "   ```\n",
    "\n",
    "3. **Move Batch to Device** ğŸ’¨  \n",
    "   Efficient GPU/CPU usage:\n",
    "   ```python\n",
    "   images = images.to(device)\n",
    "   labels = labels.to(device)\n",
    "   ```\n",
    "\n",
    "4. **Zero Gradients** ğŸ§½  \n",
    "   Clears leftover gradients from previous batch:\n",
    "   ```python\n",
    "   optimizer.zero_grad()\n",
    "   ```\n",
    "\n",
    "5. **Forward Pass** ğŸ“¤  \n",
    "   Feed data into the model:\n",
    "   ```python\n",
    "   outputs = model(images)\n",
    "   ```\n",
    "\n",
    "6. **Compute Loss** ğŸ“  \n",
    "   Compare predictions vs labels:\n",
    "   ```python\n",
    "   loss = criterion(outputs, labels)\n",
    "   ```\n",
    "\n",
    "7. **Backward Pass** ğŸ”™  \n",
    "   Backpropagate the loss:\n",
    "   ```python\n",
    "   loss.backward()\n",
    "   ```\n",
    "\n",
    "8. **Update Weights** ğŸ”§  \n",
    "   Apply gradients to update parameters:\n",
    "   ```python\n",
    "   optimizer.step()\n",
    "   ```\n",
    "\n",
    "9. **Update Metrics** ğŸ“‹  \n",
    "   Track running loss and count correct predictions:\n",
    "   ```python\n",
    "   running_loss += loss.item()\n",
    "   _, predicted = outputs.max(1)\n",
    "   correct += predicted.eq(labels).sum().item()\n",
    "   ```\n",
    "\n",
    "### âœ… End of Epoch\n",
    "\n",
    "Once all batches are done:\n",
    "\n",
    "- Compute **average loss and accuracy**.\n",
    "- Log the results with:\n",
    "  ```python\n",
    "  print(f\"Epoch {epoch+1} Results | Loss: ... | Acc: ...\")\n",
    "  ```\n",
    "- Save the model weights:\n",
    "  ```python\n",
    "  torch.save(model.state_dict(), 'entity_classifier.pth')\n",
    "  ```\n",
    "\n",
    "### ğŸ“ˆ Final Visualization\n",
    "\n",
    "After training ends, we generate a side-by-side plot:\n",
    "\n",
    "- **Left Plot:** Training Loss  \n",
    "- **Right Plot:** Training Accuracy  \n",
    "\n",
    "Both with epochs on the x-axis.\n",
    "\n",
    "> ğŸ’¾ The plot is saved as:  \n",
    "> `mobilenetv2_4class_finetune_YYYYMMDD.png`  \n",
    "> With final metrics and model details in the title & footer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa3425-a4cf-470d-b1f1-a5d39dc7453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10, device='cuda'):\n",
    "    \"\"\"Main training loop for the classification model\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        num_epochs: Number of complete passes through dataset\n",
    "        device: 'cuda' or 'cpu' for training\n",
    "        \n",
    "    Returns:\n",
    "        Trained model with updated weights\n",
    "    \"\"\"\n",
    "    print(\"Starting training process...\")\n",
    "    # Move model to target device (GPU/CPU) - critical for performance\n",
    "    model.to(device)\n",
    "    \n",
    "    # Lists to store metrics for visualization\n",
    "    train_losses = []  # Track loss per epoch\n",
    "    train_accuracies = []  # Track accuracy per epoch\n",
    "    \n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0  # Accumulate loss across batches\n",
    "        correct = 0  # Count correct predictions\n",
    "        total = 0  # Total samples processed\n",
    "        \n",
    "        # Initialize progress bar with tqdm\n",
    "        loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=True)\n",
    "        \n",
    "        # Process each batch of data\n",
    "        for batch_idx, (images, labels) in enumerate(loop):\n",
    "            # Move data to target device (GPU/CPU)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero out gradients from previous batch\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - get model predictions\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate loss between predictions and ground truth\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass - compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update model weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update training statistics\n",
    "            running_loss += loss.item()  # Add batch loss to running total\n",
    "            _, predicted = outputs.max(1)  # Get predicted class indices\n",
    "            total += labels.size(0)  # Increment total sample count\n",
    "            correct += predicted.eq(labels).sum().item()  # Count correct predictions\n",
    "            \n",
    "            # Update progress bar with current metrics\n",
    "            loop.set_postfix(\n",
    "                loss=running_loss/(batch_idx+1),  # Average loss so far\n",
    "                acc=100.*correct/total  # Current batch accuracy\n",
    "            )\n",
    "        \n",
    "        # Calculate epoch-level metrics\n",
    "        epoch_loss = running_loss / len(train_loader)  # Average epoch loss\n",
    "        epoch_acc = 100. * correct / total  # Epoch accuracy\n",
    "        \n",
    "        # Store metrics for visualization\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'Epoch {epoch+1} Results | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2f}%')\n",
    "    \n",
    "        # Save model checkpoint after each epoch\n",
    "        torch.save(model.state_dict(), 'entity_classifier.pth')\n",
    "        print('Model weights saved to entity_classifier.pth')\n",
    "    \n",
    "    # Visualize training results\n",
    "    plt.figure(figsize=(13, 5))\n",
    "\n",
    "    # Loss plot (left subplot)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, 'b', label='Loss')\n",
    "    plt.title(\"Training Loss\", pad=10)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, alpha=0.3)  # Add subtle grid\n",
    "    plt.legend(title=\"MobileNetV2 Variant\\n(Modified Head)\")\n",
    "    \n",
    "    # Accuracy plot (right subplot)\n",
    "    plt.subplot(1, 2, 2) \n",
    "    plt.plot(train_accuracies, 'r', label='Accuracy')\n",
    "    plt.title(\"Training Accuracy\", pad=10) \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(True, alpha=0.3)  # Add subtle grid\n",
    "    \n",
    "    # Main title with final metrics\n",
    "    plt.suptitle(\n",
    "        \"MobileNetV2 Fine-Tuning (Custom 4-Class Head)\\n\"\n",
    "        f\"Final Accuracy: {train_accuracies[-1]:.1f}% | Loss: {train_accuracies[-1]:.4f}\",\n",
    "        y=1.02,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "    \n",
    "    # Footer note with architecture details\n",
    "    plt.gcf().text(\n",
    "        0.5, -0.08,\n",
    "        \"Backbone: ImageNet weights (frozen) | Head: 256FC-ReLU-Dropout-4FC (trainable)\",\n",
    "        ha='center',\n",
    "        fontsize=9,\n",
    "        color='#555'\n",
    "    )\n",
    "    \n",
    "    # Final plot adjustments\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        f\"mobilenetv2_4class_finetune_{datetime.now().strftime('%Y%m%d')}.png\", \n",
    "        dpi=300,  # High resolution output\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def setup_and_train(pretrained_model, dataloader, learning_rate=0.001, num_epochs=10):\n",
    "    \"\"\"Complete training setup and execution\n",
    "    \n",
    "    Args:\n",
    "        pretrained_model: Model with initialized weights\n",
    "        dataloader: Configured DataLoader instance\n",
    "        learning_rate: Initial learning rate\n",
    "        num_epochs: Number of training epochs\n",
    "        \n",
    "    Returns:\n",
    "        Fully trained model\n",
    "    \"\"\"\n",
    "    # Define loss function - CrossEntropy for classification\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Configure optimizer - Adam with specified learning rate\n",
    "    optimizer = torch.optim.Adam(pretrained_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Auto-detect available device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f'Using device: {device.upper()}')\n",
    "    \n",
    "    # Launch training process\n",
    "    print(\"Starting model training...\")\n",
    "    trained_model = train_model(\n",
    "        model=pretrained_model,\n",
    "        train_loader=dataloader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    return trained_model\n",
    "\n",
    "# Usage\n",
    "trained_model = setup_and_train(\n",
    "    pretrained_model=model,  # Your initialized model\n",
    "    dataloader=dataloader,   # Your configured DataLoader\n",
    "    learning_rate=5e-5,      # Lower learning rate for fine-tuning\n",
    "    num_epochs=10            # Training cycles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594d8bd-8bbb-4bfe-abf6-20e7dc5eb221",
   "metadata": {},
   "source": [
    "ğŸ‰ **Success!** Our model is now fully trained and we have the metrics to prove it! ğŸ‰\n",
    "\n",
    "We've completed the training process, and our model is ready for action. By evaluating the **training loss** and **accuracy** over the epochs, we can confirm the modelâ€™s progress and how well it has learned from the data.\n",
    "\n",
    "Now that we've achieved solid performance on the training set, it's time to move on to **real-time inference**! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18419def-a282-48a8-959b-cd918ec114a4",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2048f-297f-46bf-8c78-a29c33339030",
   "metadata": {},
   "source": [
    "### Whatâ€™s next? ğŸ‘€\n",
    "Instead of continuing in a notebook, the next step is to integrate the trained model into a **PyCharm application** for real-time usage. We'll use the model to make predictions on live data, such as from a **webcam feed**. This allows us to test the modelâ€™s performance in real-world scenarios and see how it handles new, unseen data in an interactive application.\n",
    "\n",
    "Letâ€™s take the model for a spin and see how it performs when itâ€™s really put to work in a **PyCharm app**. ğŸ’»âœ¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
