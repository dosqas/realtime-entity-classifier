{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d269f49f-edda-4d60-9bb3-800e6f7d737a",
   "metadata": {},
   "source": [
    "# ğŸ§  Real-time Entity Classifier - CNN Architecture & Training\n",
    "\n",
    "This notebook handles both definition and training of a MobileNetV2-based CNN for real-time webcam classification:\n",
    "\n",
    "**Detection Categories**:\n",
    "- ğŸ± **Pet** (Felix - British Shorthair cat)\n",
    "- ğŸ‘¤ **Owner** (Sebastian)\n",
    "- ğŸ§ **Other People**\n",
    "- ğŸš« **Background/Nobody**\n",
    "\n",
    "**Key Features**:\n",
    "- âš¡ Real-time inference: ~30ms/frame (640x480 resolution)\n",
    "- ğŸ”’ Privacy-focused: All processing on-device\n",
    "- ğŸŒŸ Robust to: Lighting changes, partial occlusions\n",
    "- ğŸ—ï¸ Transfer learning: Fine-tuned from ImageNet weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7efa1e-0ec7-4f5a-8669-b8f858f56391",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d4b3a2-c363-4c1e-947b-a83b70150fb7",
   "metadata": {},
   "source": [
    "## ğŸ“¦ PyTorch & Project Imports Overview\n",
    "\n",
    "### ğŸ§  Core PyTorch Components\n",
    "\n",
    "- **`torch`**  \n",
    "  Used for tensor operations, device management (`torch.cuda.is_available()`), and model saving (`torch.save()`).\n",
    "\n",
    "- **`torch.nn`**  \n",
    "  Used for building the classifier component of our model, including `nn.Linear`, `nn.ReLU`, `nn.Dropout`, and `nn.Sequential`. Also provides `CrossEntropyLoss` with label smoothing to handle class imbalance.\n",
    "\n",
    "- **`torch.optim`**  \n",
    "  Provides the `Adam` optimizer with learning rate and weight decay parameters for training our classifier layers.\n",
    "\n",
    "- **`torch.utils.data.Dataset, DataLoader`**  \n",
    "  `Dataset` is subclassed to create our custom `VideoFrameDataset`. The `DataLoader` wraps this dataset with batch processing, shuffling, and provides an iterator for training.\n",
    "\n",
    "### ğŸ–¼ï¸ Computer Vision & Image Processing\n",
    "\n",
    "- **`torchvision.models`**  \n",
    "  Used to load a pre-trained `mobilenet_v2` as our base feature extractor, which we then modify for our specific classification task.\n",
    "    \n",
    "- **`PIL.Image`**  \n",
    "  Used to convert between NumPy arrays and PIL images for compatibility with torchvision transforms.\n",
    "\n",
    "- **`cv2` (OpenCV)**  \n",
    "  Handles video operations (`VideoCapture`) to read and extract frames from video files, and image color space conversion (`cvtColor`).\n",
    "\n",
    "- **`torchvision.transforms`**  \n",
    "  Builds image transformation pipelines for:  \n",
    "  - Standard preprocessing: resize, crop, normalization  \n",
    "  - Data augmentation: random flips, rotations, color jitter, perspective changes\n",
    "\n",
    "### ğŸ“ Data Handling & Utilities\n",
    "\n",
    "- **`os`**  \n",
    "  Checks file existence (`os.path.exists()`) and constructs file paths (`os.path.join()`).\n",
    "\n",
    "- **`glob`**  \n",
    "  Finds image files with specific extensions (`.jpg`, `.jpeg`, `.png`) within directories.\n",
    "\n",
    "- **`random`**  \n",
    "  Samples subsets of data when we have too many files (`random.sample()`), helping maintain dataset balance.\n",
    "\n",
    "### ğŸ“Š Visualization & Progress Tracking\n",
    "\n",
    "- **`matplotlib.pyplot as plt`**  \n",
    "  Creates and saves training visualization plots with loss and accuracy metrics after training completes.\n",
    "\n",
    "- **`tqdm`**  \n",
    "  Wraps the training loop to provide a progress bar with real-time metrics (loss and accuracy) during model training.\n",
    "\n",
    "These libraries together form a complete pipeline for processing video data, extracting frames, building and training a deep learning model for multi-class classification, and visualizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "280d0aba-b847-4435-bdfc-318962f346d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Core PyTorch Components\n",
    "# ======================\n",
    "import torch               # Base library for tensors, GPU ops, and autograd\n",
    "import torch.nn as nn      # Neural network layers (Linear, Conv2d, etc.)\n",
    "import torch.optim as optim  # Optimizer (Adam) for training\n",
    "from torch.utils.data import Dataset, DataLoader  # Custom datasets + efficient batching\n",
    "\n",
    "# ======================\n",
    "# Computer Vision\n",
    "# ======================\n",
    "import cv2                        # Video capture and frame processing\n",
    "from PIL import Image             # Image loading and conversion\n",
    "from torchvision import transforms  # Image preprocessing/augmentations\n",
    "from torchvision import models    # Pretrained models (MobileNetV2)\n",
    "from torchvision.models.mobilenetv2 import ( # MobileNetV2-specific:\n",
    "    MobileNet_V2_Weights                     # - Pretrained weight configurations\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# Data Pipeline\n",
    "# ======================\n",
    "import os                        # File path operations\n",
    "from glob import glob            # Pattern matching for image/video files\n",
    "import random                    # Shuffling datasets and sampling\n",
    "\n",
    "# ======================\n",
    "# Training Utilities\n",
    "# ======================\n",
    "import matplotlib.pyplot as plt  # Plotting loss/accuracy curves\n",
    "from tqdm import tqdm            # Progress bars for training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd8046-b414-4872-9661-533e52733b24",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de83e6-f723-4e42-bd24-bcd550690961",
   "metadata": {},
   "source": [
    "## MobileNetV2 Transfer Learning Architecture Explanation ğŸš€\n",
    "\n",
    "### Project Goal: 4-Way Classification System ğŸ¯\n",
    "\n",
    "This implementation adapts a pre-trained MobileNetV2 model for a specific 4-class recognition task using transfer learning principles. The model is designed to classify webcam frames into:\n",
    "\n",
    "1. **Pet** â€“ A specific British Shorthair cat named Felix ğŸ±  \n",
    "2. **Owner** â€“ Sebastian (the project creator) ğŸ‘¨  \n",
    "3. **Other Person** â€“ Any human who is not Sebastian ğŸ§  \n",
    "4. **None/Background** â€“ Empty frames with no people or pets ğŸš«\n",
    "\n",
    "### Base Model: MobileNetV2 ğŸ“±\n",
    "\n",
    "```python\n",
    "model = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1).to(device)\n",
    "```\n",
    "\n",
    "#### Why MobileNetV2? ğŸ¤”\n",
    "\n",
    "- **Efficiency** âš¡: MobileNetV2 is designed specifically for mobile and edge devices, making it computationally efficient while maintaining good accuracy\n",
    "- **Depthwise Separable Convolutions** ğŸ§©: Uses a factorized form of standard convolutions that drastically reduces parameter count and computational cost\n",
    "- **Inverted Residuals** ğŸ”„: Unlike traditional residual blocks, MobileNetV2 uses inverted residuals with linear bottlenecks, which help preserve information flow while keeping the model lightweight\n",
    "- **Pre-trained Knowledge** ğŸ§ : ImageNet pre-training provides the model with powerful feature extraction capabilities that can transfer well to new domains\n",
    "- **Size-Performance Tradeoff** âš–ï¸: Offers an excellent balance between model size (~14M parameters) and performance for real-time or resource-constrained applications\n",
    "\n",
    "### Transfer Learning Approach ğŸ”„\n",
    "\n",
    "```python\n",
    "# Freeze the feature extraction layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "```\n",
    "\n",
    "#### Benefits of Feature Freezing â„ï¸\n",
    "\n",
    "- **Training Efficiency** ğŸï¸: By freezing the convolutional backbone, we dramatically reduce the number of trainable parameters (from millions to thousands)\n",
    "- **Prevents Overfitting** ğŸ›¡ï¸: With limited training data, updating only the classifier prevents the model from overfitting to peculiarities in our small dataset\n",
    "- **Knowledge Preservation** ğŸ“š: Retains the robust feature extraction capabilities learned from ImageNet's diverse 1.2+ million images\n",
    "- **Faster Convergence** ğŸ: The classifier can adapt to the new task much more quickly when starting from well-formed features\n",
    "\n",
    "### Custom Classifier Head ğŸ‘‘\n",
    "\n",
    "```python\n",
    "model.classifier[1] = nn.Sequential(\n",
    "    nn.Linear(model.classifier[1].in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(256, 4)\n",
    ").to(device)\n",
    "```\n",
    "\n",
    "#### Classifier Design Choices ğŸ§©\n",
    "\n",
    "- **Intermediate Hidden Layer (256 neurons)** ğŸ§¬:\n",
    "  - Provides greater representational capacity than a direct mapping\n",
    "  - Allows the model to learn more complex decision boundaries between classes\n",
    "  - 256 neurons balances expressivity and computational efficiency\n",
    "  - Particularly helpful for distinguishing between similar human faces (owner vs. other person) ğŸ‘¥\n",
    "\n",
    "- **ReLU Activation** âš¡:\n",
    "  - Introduces non-linearity to capture complex relationships\n",
    "  - Mitigates the vanishing gradient problem with its non-saturating form\n",
    "  - Computationally efficient compared to tanh or sigmoid\n",
    "\n",
    "- **Dropout (0.2)** ğŸ­:\n",
    "  - Implements regularization by randomly deactivating 20% of neurons during training\n",
    "  - Prevents co-adaptation of neurons (neurons becoming too dependent on each other)\n",
    "  - Forces the network to learn redundant representations, improving generalization\n",
    "  - Rate of 0.2 is conservative, providing regularization while preserving most information flow\n",
    "  - Particularly important for this task since the dataset likely contains many similar frames of the same subjects\n",
    "\n",
    "- **Output Layer (4 neurons)** ğŸ¬:\n",
    "  - One neuron per class (pet, owner, other person, background)\n",
    "  - Used with CrossEntropyLoss which applies softmax internally to produce probabilities\n",
    "\n",
    "### Weight Initialization Strategy ğŸ²\n",
    "\n",
    "```python\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "```\n",
    "\n",
    "#### Xavier/Glorot Initialization Benefits âœ¨\n",
    "\n",
    "- **Variance Control** ğŸ“Š: Maintains the variance of activations and gradients across layers\n",
    "- **Prevents Signal Vanishing/Exploding** ğŸ’¥: Scaling weights based on the layer size helps signal propagate effectively\n",
    "- **Uniform Distribution** ğŸ“ˆ: Draws weights from a uniform distribution within a carefully calculated range\n",
    "- **Faster Convergence** ğŸš€: Well-initialized weights allow the model to reach optimal regions more quickly\n",
    "- **Zero Bias Initialization** 0ï¸âƒ£: Starting biases at zero is a standard practice that works well with ReLU when batch normalization isn't used\n",
    "\n",
    "### Model Deployment ğŸš€\n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "```\n",
    "\n",
    "#### Hardware Acceleration ğŸ’»\n",
    "\n",
    "- **Dynamic Device Selection** ğŸ”: Code automatically selects GPU if available, falling back to CPU if necessary\n",
    "- **Memory Management** ğŸ’¾: Moving the model to the appropriate device ensures efficient memory usage\n",
    "- **Computation Speed** âš¡: Running on GPU can offer 10-100x speedup for neural network operations\n",
    "\n",
    "### Architecture Summary ğŸ“\n",
    "\n",
    "This architecture exemplifies modern transfer learning best practices by:\n",
    "\n",
    "1. Leveraging a pre-trained, efficient CNN architecture ğŸ—ï¸\n",
    "2. Freezing feature extraction layers to preserve learned representations â„ï¸\n",
    "3. Implementing a purpose-built classifier for the specific task ğŸ¯\n",
    "4. Using appropriate regularization techniques to prevent overfitting ğŸ›¡ï¸\n",
    "5. Applying proven weight initialization strategies for faster convergence ğŸ\n",
    "\n",
    "The resulting model balances computational efficiency with classification performance, making it suitable for deployment in resource-constrained environments while still maintaining high accuracy for this pet and person recognition task. ğŸ¤–ğŸ‘\n",
    "\n",
    "### Application-Specific Advantages ğŸŒŸ\n",
    "\n",
    "For this specific pet/owner recognition task:\n",
    "\n",
    "1. **Fine-Grained Recognition** ğŸ”: The model can learn subtle differences between a specific cat (Felix) and other animals, or between the owner and other people\n",
    "3. **Real-time Processing** â±ï¸: MobileNetV2's efficiency enables real-time classification on webcam streams\n",
    "4. **Low Resource Requirements** ğŸ’ª: The architecture can run on modest hardware like laptops without dedicated GPUs\n",
    "5. **Quick Training** ğŸï¸: By using transfer learning, the model can be trained with relatively few examples of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69abfb33-9ce8-4b30-b3bf-d0047046fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n",
      "Classifier structure:\n",
      "Sequential(\n",
      "  (0): Dropout(p=0.2, inplace=False)\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# Model Definition\n",
    "# ======================\n",
    "# Set computation device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained MobileNetV2 with ImageNet weights\n",
    "# MobileNetV2 is chosen for its efficiency, performance, and lightweight nature\n",
    "model = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1).to(device)\n",
    "\n",
    "# Freeze the feature extraction layers to preserve pre-trained knowledge\n",
    "# This implements transfer learning - we only train the classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier with our custom head for 4-class classification\n",
    "# Architecture: Input features â†’ 256 neurons â†’ ReLU â†’ Dropout â†’ 4 output classes\n",
    "model.classifier[1] = nn.Sequential(\n",
    "    nn.Linear(model.classifier[1].in_features, 256),  # Hidden layer with 256 neurons\n",
    "    nn.ReLU(),                                        # Non-linearity\n",
    "    nn.Dropout(0.2),                                  # Regularization to prevent overfitting\n",
    "    nn.Linear(256, 4)                                 # Output layer for our 4 classes\n",
    ").to(device)\n",
    "\n",
    "# Define weight initialization function for better convergence\n",
    "# Xavier/Glorot initialization helps control variance of activations across layers\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)  # Uniform distribution within proper bounds\n",
    "        nn.init.zeros_(m.bias)             # Initialize biases to zero\n",
    "\n",
    "# Apply weight initialization to our classifier only\n",
    "model.classifier[1].apply(init_weights)\n",
    "\n",
    "# Verify model setup\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(\"Classifier structure:\")\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d361f0a-a50a-4360-9844-50151d65ad6c",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029fb06-3a55-45ab-91a3-02b06e8a6a00",
   "metadata": {},
   "source": [
    "## Training Configuration: Optimizer and Loss Function ğŸ› ï¸\n",
    "\n",
    "### Optimizer: Adam with Selective Training ğŸ¯\n",
    "\n",
    "```python\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(trainable_params, lr=0.001, weight_decay=1e-5)\n",
    "```\n",
    "\n",
    "#### Parameter Selection Strategy ğŸ”\n",
    "\n",
    "We use `filter(lambda p: p.requires_grad, model.parameters())` to select only trainable parameters. This is critical for our transfer learning approach:\n",
    "\n",
    "- **Efficiency Boost** âš¡: By optimizing only the classifier parameters ( ~few thousand) instead of the entire model ( ~14 million), we drastically reduce computation time and memory requirements\n",
    "- **Focused Learning** ğŸ”­: Updates are restricted to the task-specific components (the classifier), leaving the pre-trained feature extractor untouched\n",
    "- **Lambda Function Elegance** ğŸ’»: The lambda function creates a clean, one-line filter that selects parameters where `requires_grad=True`\n",
    "\n",
    "#### Why Adam? ğŸ¤”\n",
    "\n",
    "Adam (Adaptive Moment Estimation) combines the benefits of two other extensions of stochastic gradient descent:\n",
    "\n",
    "- **Adaptive Learning Rates** ğŸ“Š: Automatically adjusts learning rates for each parameter based on historical gradients\n",
    "- **Momentum** ğŸï¸: Accelerates convergence by adding a fraction of the previous update direction\n",
    "- **RMSProp Integration** ğŸ“‰: Adapts learning rates based on the average of recent magnitudes of gradients\n",
    "- **Sparse Gradient Handling** ğŸŒµ: Performs well even when gradients are sparse or noisy\n",
    "\n",
    "#### Hyperparameter Choices ğŸ›ï¸\n",
    "\n",
    "- **Learning Rate (5e-5)** ğŸ: \n",
    "  - A smaller learning rate typically used for fine-tuning pre-trained models.\n",
    "  - Helps ensure that the model updates its weights gradually, preventing large, unstable changes.\n",
    "  - Useful when working with pre-trained weights, as it allows the model to make subtle adjustments without drastically altering the learned features.\n",
    "  - Small enough to avoid overshooting the optimal solution, promoting stable convergence, especially in later training stages.\n",
    "\n",
    "- **Weight Decay (1e-5)** ğŸŒ±: \n",
    "  - Implements L2 regularization by penalizing large weights\n",
    "  - Helps prevent overfitting by encouraging the model to use smaller weights\n",
    "  - Value of 1e-5 is conservative, providing gentle regularization\n",
    "  - Particularly valuable for our problem where limited training data could lead to overfitting\n",
    "\n",
    "### Loss Function: CrossEntropyLoss with Label Smoothing ğŸ“Š\n",
    "\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "```\n",
    "\n",
    "#### Why CrossEntropyLoss? ğŸ¯\n",
    "\n",
    "CrossEntropyLoss is the standard choice for multi-class classification problems:\n",
    "\n",
    "- **Softmax Integration** ğŸ”„: Internally applies softmax to convert logits to probabilities\n",
    "- **Class Probability** ğŸ“Š: Measures the difference between predicted probability distribution and the target distribution\n",
    "- **Numerical Stability** ğŸ›¡ï¸: Implements log-softmax and negative log-likelihood in a numerically stable way\n",
    "- **Single-Label Focus** ğŸ·ï¸: Optimized for problems where each example belongs to exactly one class\n",
    "\n",
    "#### Label Smoothing (0.1) ğŸ¥¤\n",
    "\n",
    "Label smoothing is a regularization technique that modifies the target distribution:\n",
    "\n",
    "- **Target Softening** â˜ï¸: Instead of using hard labels (0,0,1,0), it creates soft targets (0.03,0.03,0.9,0.03)\n",
    "- **Confidence Penalty** ğŸ“‰: Discourages the model from becoming too confident in its predictions\n",
    "- **Overfitting Prevention** ğŸ›¡ï¸: Makes the model less likely to memorize training data noise or errors\n",
    "- **Class Imbalance Handling** âš–ï¸: Particularly valuable for our use case with potential class imbalance (may have more background frames than pet frames)\n",
    "- **Value Selection (0.1)** ğŸšï¸: \n",
    "  - 0.1 is a moderate smoothing value that provides regularization benefits\n",
    "  - High enough to prevent overconfidence\n",
    "  - Low enough to maintain class separation\n",
    "  \n",
    "### Combined Effect on Training Dynamics ğŸ”„\n",
    "\n",
    "Together, these choices create a training configuration that:\n",
    "\n",
    "1. **Focuses computational effort** on adapting the classifier to our specific classes\n",
    "2. **Adapts learning dynamically** based on gradient behavior during training\n",
    "3. **Regularizes from multiple angles** (weight decay and label smoothing) to prevent overfitting\n",
    "4. **Improves generalization** especially in the face of class imbalance or limited training data\n",
    "5. **Accelerates convergence** compared to standard SGD or simpler optimizers\n",
    "\n",
    "This configuration represents modern deep learning best practices for transfer learning on classification tasks with limited, potentially imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba882d62-0c1f-44c9-ac41-7752c5b79b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer (only train classifier parameters)\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())  \n",
    "# Filter out the parameters that require gradients (only the custom classifier will be trained)\n",
    "# This is efficient because we freeze the feature extraction layers, so we don't need to train them again.\n",
    "optimizer = optim.Adam(trainable_params, lr=5e-5, weight_decay=1e-5)  \n",
    "# Adam optimizer is chosen for adaptive learning rates, which helps to converge faster and more efficiently\n",
    "# lr=5e-5 (=0.00005): A learning rate of 5e-5 is a commonly used starting point for fine-tuning pre-trained models.\n",
    "# It allows the model to adjust the new classifier layers without drastically altering the pre-trained weights.\n",
    "# It's generally small enough to avoid overfitting but large enough to enable the classifier to learn efficiently.\n",
    "# weight_decay=1e-5: L2 regularization helps prevent overfitting by penalizing large weights (helps generalization).\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  \n",
    "# CrossEntropyLoss is commonly used for multi-class classification tasks, as it combines softmax and negative log-likelihood loss\n",
    "# label_smoothing=0.1: This technique reduces the confidence of the model when predicting the target class, \n",
    "# making it less likely to overfit on noisy or incorrect labels and improving generalization, especially for imbalanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4363382a-8f8a-4992-91ad-1ad931ad1f6b",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ed4c8-db25-479a-80a9-eb33265be1e9",
   "metadata": {},
   "source": [
    "## ğŸ“¦ DataLoader and Dataset Setup\n",
    "\n",
    "Before training our model, we need to properly load and preprocess the image data. This is handled using PyTorchâ€™s `ImageFolder` and `DataLoader` utilities.\n",
    "\n",
    "### ğŸ–¼ï¸ Image Preprocessing\n",
    "We define a series of transformations to ensure consistency and improve model performance:\n",
    "- **Resize**: All images are resized to a fixed dimension of **224x224** pixels. This is a common size for CNNs and ensures a uniform input shape.\n",
    "- **ToTensor**: Images are converted into PyTorch tensors, enabling them to be processed by the model.\n",
    "- **Normalize**: Pixel values are normalized using the mean and standard deviation from the ImageNet dataset:\n",
    "  - Mean: `[0.485, 0.456, 0.406]`\n",
    "  - Standard Deviation: `[0.229, 0.224, 0.225]`\n",
    "  This helps with model convergence and consistency.\n",
    "\n",
    "### ğŸ—‚ï¸ Dataset Structure\n",
    "The dataset is organized using folders representing each class. Each subdirectory (e.g., `nobody`, `pet`, `owner`, `other_person`) contains images specific to that class. PyTorchâ€™s `ImageFolder` automatically maps these folders to class labels.\n",
    "\n",
    "### ğŸ› ï¸ DataLoader Creation\n",
    "We create two `DataLoader` objects:\n",
    "- **Training Loader**: Loads the dataset in shuffled batches of 32 images to ensure randomness and improve generalization.\n",
    "- **Validation Loader**: Loads the validation data without shuffling, maintaining the order for consistent evaluation.\n",
    "\n",
    "The `num_workers=4` setting allows data loading to happen in parallel for better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7888ee1-ff96-4f93-b548-823b7720630e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n",
      "\n",
      "==================================================\n",
      "STARTING DATA INDEXING\n",
      "==================================================\n",
      "\n",
      "--------------------\n",
      "INDEXING OWNER VIDEOS\n",
      "--------------------\n",
      "\n",
      "Indexing: ../data/owner/owner.mp4\n",
      "Indexing 1750 frames from 3150 total frames\n",
      "Indexed 1750 frames from ../data/owner/owner.mp4\n",
      "\n",
      "--------------------\n",
      "INDEXING PET VIDEOS\n",
      "--------------------\n",
      "\n",
      "Indexing: ../data/pet/pet.mp4\n",
      "Indexing 915 frames from 915 total frames\n",
      "Indexed 915 frames from ../data/pet/pet.mp4\n",
      "\n",
      "--------------------\n",
      "INDEXING FACE IMAGES FROM DIRECTORY\n",
      "--------------------\n",
      "Directory: ../data/other_people/other_people\n",
      "Found 7219 face images\n",
      "Randomly sampling 2500 faces...\n",
      "Indexed 100/2500 face images\n",
      "Indexed 200/2500 face images\n",
      "Indexed 300/2500 face images\n",
      "Indexed 400/2500 face images\n",
      "Indexed 500/2500 face images\n",
      "Indexed 600/2500 face images\n",
      "Indexed 700/2500 face images\n",
      "Indexed 800/2500 face images\n",
      "Indexed 900/2500 face images\n",
      "Indexed 1000/2500 face images\n",
      "Indexed 1100/2500 face images\n",
      "Indexed 1200/2500 face images\n",
      "Indexed 1300/2500 face images\n",
      "Indexed 1400/2500 face images\n",
      "Indexed 1500/2500 face images\n",
      "Indexed 1600/2500 face images\n",
      "Indexed 1700/2500 face images\n",
      "Indexed 1800/2500 face images\n",
      "Indexed 1900/2500 face images\n",
      "Indexed 2000/2500 face images\n",
      "Indexed 2100/2500 face images\n",
      "Indexed 2200/2500 face images\n",
      "Indexed 2300/2500 face images\n",
      "Indexed 2400/2500 face images\n",
      "Indexed 2500/2500 face images\n",
      "Finished indexing 2500 face images\n",
      "\n",
      "--------------------\n",
      "INDEXING BACKGROUND DATA\n",
      "--------------------\n",
      "\n",
      "Indexing background video: ../data/background/background.mp4\n",
      "Indexing 1750 frames from 1765 total frames\n",
      "Indexed 1750 frames from ../data/background/background.mp4\n",
      "\n",
      "==================================================\n",
      "DATA INDEXING COMPLETE\n",
      "==================================================\n",
      "\n",
      "Dataset created with 34575 samples\n",
      "Class distribution:\n",
      "- owner: 8750 samples (25.3%)\n",
      "- pet: 4575 samples (13.2%)\n",
      "- other: 12500 samples (36.2%)\n",
      "- background: 8750 samples (25.3%)\n",
      "Testing dataloader...\n",
      "Batch 0: x shape = torch.Size([16, 3, 224, 224]), y shape = torch.Size([16])\n",
      "Batch 1: x shape = torch.Size([16, 3, 224, 224]), y shape = torch.Size([16])\n",
      "Batch 2: x shape = torch.Size([16, 3, 224, 224]), y shape = torch.Size([16])\n",
      "Dataloader test successful!\n"
     ]
    }
   ],
   "source": [
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, video_paths, label_mapping, transform=None, \n",
    "                 augment=False, num_augments=3, bg_paths=None, \n",
    "                 face_image_dir=None, max_frames_per_video=300):\n",
    "        self.video_paths = video_paths\n",
    "        self.label_mapping = label_mapping\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.num_augments = num_augments\n",
    "        self.bg_paths = bg_paths or []\n",
    "        self.face_image_dir = face_image_dir\n",
    "        self.max_frames_per_video = max_frames_per_video\n",
    "        \n",
    "        # Instead of storing frames, store frame references\n",
    "        self.frame_sources = []  # Will contain (source_type, path, frame_idx, label)\n",
    "        self.class_counts = {name: 0 for name in label_mapping}\n",
    "        \n",
    "        self._index_all_data()\n",
    "        \n",
    "        if len(self.frame_sources) == 0:\n",
    "            raise ValueError(\"No valid frames were indexed. Please check your input paths.\")\n",
    "        \n",
    "        total_samples = len(self.frame_sources)\n",
    "        if self.augment:\n",
    "            total_samples *= (1 + self.num_augments)\n",
    "            \n",
    "        print(f\"\\nDataset created with {total_samples} samples\")\n",
    "        print(\"Class distribution:\")\n",
    "        for name, idx in label_mapping.items():\n",
    "            count = self.class_counts[name] * (1 + self.num_augments if self.augment else 1)\n",
    "            print(f\"- {name}: {count} samples ({count/total_samples:.1%})\")\n",
    "\n",
    "    def _index_all_data(self):\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STARTING DATA INDEXING\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Index owner video\n",
    "        if \"owner\" in self.video_paths:\n",
    "            print(\"\\n\" + \"-\"*20)\n",
    "            print(\"INDEXING OWNER VIDEOS\")\n",
    "            print(\"-\"*20)\n",
    "            for path in self.video_paths[\"owner\"]:\n",
    "                print(f\"\\nIndexing: {path}\")\n",
    "                self._index_video(path, self.label_mapping[\"owner\"], \"owner\")\n",
    "\n",
    "        # Index pet video\n",
    "        if \"pet\" in self.video_paths:\n",
    "            print(\"\\n\" + \"-\"*20)\n",
    "            print(\"INDEXING PET VIDEOS\")\n",
    "            print(\"-\"*20)\n",
    "            for path in self.video_paths[\"pet\"]:\n",
    "                print(f\"\\nIndexing: {path}\")\n",
    "                self._index_video(path, self.label_mapping[\"pet\"], \"pet\")\n",
    "\n",
    "        # Index face images if directory provided\n",
    "        if self.face_image_dir:\n",
    "            print(\"\\n\" + \"-\"*20)\n",
    "            print(\"INDEXING FACE IMAGES FROM DIRECTORY\")\n",
    "            print(\"-\"*20)\n",
    "            print(f\"Directory: {self.face_image_dir}\")\n",
    "            \n",
    "            face_paths = glob(os.path.join(self.face_image_dir, \"*.jpg\")) + \\\n",
    "                        glob(os.path.join(self.face_image_dir, \"*.jpeg\")) + \\\n",
    "                        glob(os.path.join(self.face_image_dir, \"*.png\"))\n",
    "            \n",
    "            if face_paths:\n",
    "                print(f\"Found {len(face_paths)} face images\")\n",
    "                # Sample exactly 350 faces (or all if less available)\n",
    "                sample_size = min(2500, len(face_paths))\n",
    "                print(f\"Randomly sampling {sample_size} faces...\")\n",
    "                sampled_paths = random.sample(face_paths, sample_size)\n",
    "                \n",
    "                for i, img_path in enumerate(sampled_paths, 1):\n",
    "                    if i % 100 == 0:  # Print progress every 100 images\n",
    "                        print(f\"Indexed {i}/{len(sampled_paths)} face images\")\n",
    "                    self._index_image(img_path, self.label_mapping[\"other\"], \"other\")\n",
    "                print(f\"Finished indexing {len(sampled_paths)} face images\")\n",
    "            else:\n",
    "                print(f\"No face images found in {self.face_image_dir}\")\n",
    "\n",
    "        # Index background data\n",
    "        if self.bg_paths:\n",
    "            print(\"\\n\" + \"-\"*20)\n",
    "            print(\"INDEXING BACKGROUND DATA\")\n",
    "            print(\"-\"*20)\n",
    "            for path in self.bg_paths:\n",
    "                if os.path.isdir(path):\n",
    "                    print(f\"\\nIndexing background images from: {path}\")\n",
    "                    img_paths = [\n",
    "                        f for f in glob(os.path.join(path, \"*\")) \n",
    "                        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "                    ]\n",
    "                    if img_paths:\n",
    "                        sample_size = min(400, len(img_paths))\n",
    "                        print(f\"Found {len(img_paths)} images, sampling {sample_size}...\")\n",
    "                        sampled_paths = random.sample(img_paths, sample_size)\n",
    "                        for i, img_path in enumerate(sampled_paths, 1):\n",
    "                            if i % 100 == 0:\n",
    "                                print(f\"Indexed {i}/{len(sampled_paths)} background images\")\n",
    "                            self._index_image(img_path, self.label_mapping[\"background\"], \"background\")\n",
    "                    else:\n",
    "                        print(f\"No valid images found in {path}\")\n",
    "                else:\n",
    "                    print(f\"\\nIndexing background video: {path}\")\n",
    "                    self._index_video(path, self.label_mapping[\"background\"], \"background\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"DATA INDEXING COMPLETE\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    def _index_video(self, path, label, class_name):\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Error: File not found - {path}\")\n",
    "                return\n",
    "    \n",
    "            cap = cv2.VideoCapture(path)\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Error: Could not open video - {path}\")\n",
    "                return\n",
    "    \n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            if total_frames == 0:\n",
    "                print(f\"Warning: Video has 0 frames - {path}\")\n",
    "                cap.release()\n",
    "                return\n",
    "    \n",
    "            # Calculate exact number of frames to sample\n",
    "            target_frames = min(self.max_frames_per_video, total_frames)\n",
    "            step = max(1, total_frames // target_frames)\n",
    "            \n",
    "            print(f\"Indexing {target_frames} frames from {total_frames} total frames\")\n",
    "            \n",
    "            frames_indexed = 0\n",
    "            frame_indices = list(range(0, total_frames, step))[:target_frames]\n",
    "            \n",
    "            # Just store references to the frames\n",
    "            for frame_idx in frame_indices:\n",
    "                self.frame_sources.append((\"video\", path, frame_idx, label))\n",
    "                frames_indexed += 1\n",
    "                self.class_counts[class_name] += 1\n",
    "    \n",
    "            cap.release()\n",
    "            print(f\"Indexed {frames_indexed} frames from {path}\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error indexing video {path}: {str(e)}\")\n",
    "\n",
    "    def _index_image(self, img_path, label, class_name):\n",
    "        try:\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Error: File not found - {img_path}\")\n",
    "                return\n",
    "\n",
    "            # Just store reference to the image\n",
    "            self.frame_sources.append((\"image\", img_path, 0, label))\n",
    "            self.class_counts[class_name] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error indexing image {img_path}: {str(e)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        base_len = len(self.frame_sources)\n",
    "        if self.augment:\n",
    "            return base_len * (1 + self.num_augments)\n",
    "        return base_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Calculate which frame and whether this is an augmented version\n",
    "            if self.augment:\n",
    "                base_idx = idx // (1 + self.num_augments)\n",
    "                aug_version = idx % (1 + self.num_augments)\n",
    "            else:\n",
    "                base_idx = idx\n",
    "                aug_version = 0\n",
    "                \n",
    "            # Get the frame source info\n",
    "            source_type, path, frame_idx, label = self.frame_sources[base_idx]\n",
    "            \n",
    "            # Load the frame on demand\n",
    "            if source_type == \"video\":\n",
    "                frame = self._load_video_frame(path, frame_idx)\n",
    "            else:  # image\n",
    "                frame = self._load_image(path)\n",
    "                \n",
    "            # Apply augmentation if needed\n",
    "            if aug_version > 0:\n",
    "                frame = self._augment_frame(frame)\n",
    "            \n",
    "            # Apply standard transformation\n",
    "            if self.transform:\n",
    "                if not isinstance(frame, Image.Image):\n",
    "                    frame = Image.fromarray(frame)\n",
    "                frame = self.transform(frame)\n",
    "            \n",
    "            return frame, torch.tensor(label, dtype=torch.long)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting item {idx}: {str(e)}\")\n",
    "            # Return a simple default item instead of recursion\n",
    "            dummy_data = torch.zeros((3, 224, 224))\n",
    "            return dummy_data, torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "    def _load_video_frame(self, video_path, frame_idx):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if not ret or frame is None:\n",
    "            raise ValueError(f\"Failed to load frame {frame_idx} from {video_path}\")\n",
    "            \n",
    "        return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    def _load_image(self, img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Failed to load image {img_path}\")\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    def _augment_frame(self, frame):\n",
    "        augmenter = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
    "            ], p=0.8),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        ])\n",
    "        return augmenter(frame)\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths config\n",
    "    label_mapping = {\n",
    "        \"owner\": 0,    # You\n",
    "        \"pet\": 1,      # Your cat\n",
    "        \"other\": 2,    # Other people\n",
    "        \"background\": 3  # Empty background\n",
    "    }\n",
    "\n",
    "    video_paths = {\n",
    "        \"owner\": [\"../data/owner/owner.mp4\"],\n",
    "        \"pet\": [\"../data/pet/pet.mp4\"],\n",
    "    }\n",
    "\n",
    "    bg_paths = [\n",
    "        \"../data/background/background.mp4\"\n",
    "    ]\n",
    "\n",
    "    face_image_dir = \"../data/other_people/other_people\"\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        print(\"Creating dataset...\")\n",
    "        dataset = VideoFrameDataset(\n",
    "            video_paths=video_paths,\n",
    "            label_mapping=label_mapping,\n",
    "            transform=transform,\n",
    "            augment=True,\n",
    "            num_augments=4,\n",
    "            bg_paths=bg_paths,\n",
    "            face_image_dir=face_image_dir,\n",
    "            max_frames_per_video=1750\n",
    "        )\n",
    "        \n",
    "        # Start with fewer workers for testing\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=16,  # Increased batch size\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # Start with 0 workers for testing\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "        \n",
    "        print(\"Testing dataloader...\")\n",
    "        for i, (x, y) in enumerate(dataloader):\n",
    "            print(f\"Batch {i}: x shape = {x.shape}, y shape = {y.shape}\")\n",
    "            if i == 2:  # Limit output\n",
    "                break\n",
    "        print(\"Dataloader test successful!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940fee7e-1180-4e67-b739-123dfb0de7f3",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c00727-fb47-4b0a-953c-66fcbc7d0e5a",
   "metadata": {},
   "source": [
    "## ğŸ§  Model Training Loop Explained\n",
    "\n",
    "This section explains the logic behind training our deep learning model using PyTorch.\n",
    "\n",
    "### ğŸ” Epochs\n",
    "\n",
    "We train the model over **100 epochs** â€” one epoch is a complete pass over the entire training dataset. Repeating this allows the model to learn gradually and improve its performance over time.\n",
    "\n",
    "### ğŸ“Š Metrics Tracked\n",
    "\n",
    "During training, we monitor two key metrics every epoch:\n",
    "- **Loss** (ğŸ“‰): Measures how far off the modelâ€™s predictions are from the actual labels.\n",
    "- **Accuracy** (âœ…): Tells us how many predictions were correct out of the total.\n",
    "\n",
    "These are stored in `train_losses` and `train_accuracies` lists to help us visualize progress later.\n",
    "\n",
    "### ğŸ”„ Training Steps Per Epoch\n",
    "\n",
    "Each epoch consists of several steps that happen for every batch of images:\n",
    "\n",
    "1. **Set model to training mode** ğŸ‹ï¸  \n",
    "   Enables dropout and batch normalization, which behave differently during training.\n",
    "\n",
    "2. **Loop through batches of data** ğŸ“¦  \n",
    "   We use `tqdm` to wrap our DataLoader, which gives us a nice real-time progress bar.\n",
    "\n",
    "3. **Zero out gradients** ğŸ§½  \n",
    "   We reset gradients using `optimizer.zero_grad()` so that past gradient values donâ€™t accumulate.\n",
    "\n",
    "4. **Forward pass** ğŸ“¤  \n",
    "   The input images are passed through the model to generate predictions.\n",
    "\n",
    "5. **Calculate loss** ğŸ“  \n",
    "   We use a loss function (CrossEntropyLoss) to compute how wrong the model was.\n",
    "\n",
    "6. **Backward pass** ğŸ§®  \n",
    "   We call `.backward()` on the loss to compute gradients of the model parameters.\n",
    "\n",
    "7. **Update weights** ğŸ”§  \n",
    "   The optimizer (Adam) updates the model parameters based on the gradients with `optimizer.step()`.\n",
    "\n",
    "8. **Track loss and accuracy** ğŸ§¾  \n",
    "   We add the loss to a running total and count how many predictions were correct.\n",
    "\n",
    "### ğŸ§® After Each Epoch\n",
    "\n",
    "Once all batches are processed in an epoch:\n",
    "- We calculate the **average loss** and **accuracy** for that epoch.\n",
    "- We print this info to the console.\n",
    "- We append the values to our tracking lists for later use.\n",
    "\n",
    "### ğŸ’¾ Saving the Model\n",
    "\n",
    "At the end of training, we save the learned weights of our model to a file `entity_classifier.pth`. This allows us to reuse the model later without retraining it from scratch! ğŸ’¡\n",
    "\n",
    "### ğŸ“ˆ Plotting Metrics\n",
    "\n",
    "We use `matplotlib` to generate two side-by-side plots:\n",
    "- **Training Loss per Epoch** (in red)\n",
    "- **Training Accuracy per Epoch** (in green)\n",
    "\n",
    "These plots visually show how well the model learned over time â€” ideally, loss should decrease while accuracy increases! ğŸ“‰ğŸ“ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa3425-a4cf-470d-b1f1-a5d39dc7453d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "ğŸ”¥ Starting training process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2161/2161 [49:12<00:00,  1.37s/it, acc=95, loss=0.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.1600 | Acc: 95.01%\n",
      "Model saved to entity_classifier.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   4%|â–ˆâ–‹                                           | 84/2161 [01:42<35:00,  1.01s/it, acc=97.6, loss=0.0668]"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10, device='cuda'):\n",
    "    print(\"ğŸ”¥ Starting training process...\")\n",
    "    # Move model to the specified device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Lists to store metrics for plotting\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Initialize progress bar\n",
    "        loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=True)\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(loop):\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - simplified since we're no longer dealing with lists of frames\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            loop.set_postfix(\n",
    "                loss=running_loss/(batch_idx+1),\n",
    "                acc=100.*correct/total\n",
    "            )\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2f}%')\n",
    "    \n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), 'entity_classifier.pth')\n",
    "        print('Model saved to entity_classifier.pth')\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_results.png')  # Save the plot before showing it\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Complete setup and training code\n",
    "def setup_and_train(pretrained_model, dataloader, learning_rate=0.001, num_epochs=10):\n",
    "    # Define loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Configure optimizer\n",
    "    optimizer = torch.optim.Adam(pretrained_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Set device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting model training...\")\n",
    "    trained_model = train_model(\n",
    "        model=pretrained_model,\n",
    "        train_loader=dataloader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    return trained_model\n",
    "\n",
    "trained_model = setup_and_train(\n",
    "    pretrained_model=model,  # Your model instance (e.g., models.resnet18())\n",
    "    dataloader=dataloader,   # Your DataLoader from VideoFrameDataset\n",
    "    learning_rate=5e-5,          # Optional (default is 0.001)\n",
    "    num_epochs=10                 # Optional (default is 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594d8bd-8bbb-4bfe-abf6-20e7dc5eb221",
   "metadata": {},
   "source": [
    "ğŸ‰ **Success!** Our model is now fully trained and we have the metrics to prove it! ğŸ‰\n",
    "\n",
    "We've completed the training process, and our model is ready for action. By evaluating the **training loss** and **accuracy** over the epochs, we can confirm the modelâ€™s progress and how well it has learned from the data.\n",
    "\n",
    "Now that we've achieved solid performance on the training set, it's time to move on to **real-time inference**! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18419def-a282-48a8-959b-cd918ec114a4",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2048f-297f-46bf-8c78-a29c33339030",
   "metadata": {},
   "source": [
    "### Whatâ€™s next? ğŸ‘€\n",
    "Instead of continuing in a notebook, the next step is to integrate the trained model into a **PyCharm application** for real-time usage. We'll use the model to make predictions on live data, such as from a **webcam feed**. This allows us to test the modelâ€™s performance in real-world scenarios and see how it handles new, unseen data in an interactive application.\n",
    "\n",
    "Letâ€™s take the model for a spin and see how it performs when itâ€™s really put to work in a **PyCharm app**. ğŸ’»âœ¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
